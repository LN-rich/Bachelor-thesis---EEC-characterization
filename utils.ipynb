{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e0d934-6c48-4bcf-b318-834a4eba490b",
   "metadata": {},
   "source": [
    " Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c142e19-a9cc-4c23-bde6-04945c971cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import scipy as sci\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "from itertools import chain\n",
    "import h5py\n",
    "import scipy.sparse as sparse\n",
    "import anndata as ad\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from matplotlib.pyplot import rc_context\n",
    "import seaborn as sb\n",
    "#from plotnine import *\n",
    "from adjustText import adjust_text\n",
    "#import pegasus as pg\n",
    "\n",
    "# Analysis\n",
    "import scanpy as sc\n",
    "\n",
    "#import snapatac2 as snap\n",
    "import pysam\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6cc69-d43e-4094-ba83-404848b24cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_R(R_path):\n",
    "    import os\n",
    "    os.environ['R_HOME'] = R_path\n",
    "    ## R settings\n",
    "\n",
    "    ### Ignore R warning messages\n",
    "    #### Note: this can be commented out to get more verbose R output\n",
    "    rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n",
    "\n",
    "    ### Automatically convert rpy2 outputs to pandas dataframes\n",
    "    pandas2ri.activate()\n",
    "    anndata2ri.activate()\n",
    "    %load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96732ffa-e4cd-41c0-a524-7fad613d67d6",
   "metadata": {},
   "source": [
    "## read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbd8ca-aacb-409f-86ba-6afc405f0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_to_adata(path):\n",
    "    adata = sc.read_10x_h5(path)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e2883-a27a-4240-95af-855010f1e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_to_mudata(path):\n",
    "    adata = mu.read_10x_h5(path)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b0034-91a0-41f7-a814-a36503b2d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5ad_to_adata(path):\n",
    "    adata = sc.read(path)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa30805-6852-4300-aaa5-30b8222522f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5mu_to_mudata(path):\n",
    "    adata = mu.read(path)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10c11b-e2a3-4e87-8b01-b46615053ebb",
   "metadata": {},
   "source": [
    "## add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ebabd-0749-4ff6-a16e-235ffad5b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel_metadata(path, ix_col=None):\n",
    "    metadata = pd.read_excel(path, index_col=ix_col)\n",
    "    #print(metadata)\n",
    "    return metadata              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a454a-362f-4595-925c-5232ae74cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(metadata_df,adata):\n",
    "    sample_id = adata.obs['sample'][0]\n",
    "    \n",
    "    # condition with df.values property\n",
    "    mask = metadata_df['Link_id'].values == sample_id\n",
    "\n",
    "    # new dataframe\n",
    "    df_new = metadata_df[mask].T\n",
    "    #print(df_new)\n",
    "    for col, value in df_new.iterrows():\n",
    "        #print(col, value)\n",
    "        adata.obs[col]=value.values[0]\n",
    "    \n",
    "    return adata\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c88d80-e393-4933-976e-5089a064414d",
   "metadata": {},
   "source": [
    "## ambient detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e0dc6-53d2-4d81-b6b6-e267e3e9bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ambient_threshold(adata, threshold=0.0005, lower_limit=0.0001, upper_limit=0.002, bins=60, kde=True):\n",
    "    sample = adata.obs['sample'][0]\n",
    "    \n",
    "    with rc_context({'figure.figsize': (8, 3)}):\n",
    "        sb.distplot(adata.var['ambient_genes_values'][(adata.var['ambient_genes_values'] > lower_limit) & (adata.var['ambient_genes_values'] < upper_limit)], kde=kde, bins=bins)\n",
    "        plt.axvline(threshold, 0, 1)\n",
    "        plt.title(label=f'Ambient Genes Threshold {sample} (' + str(len(adata.var['ambient_genes_values'][adata.var['ambient_genes_values'] > threshold])) + ' Genes)', fontweight='bold')\n",
    "        plt.show()\n",
    "    \n",
    "    #adata.var['is_ambient'] = pd.Categorical(adata.var['ambient_genes_values'] > threshold) already done in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefilter_barcodes_mdata(mdata, barcodes=None, plot=True):\n",
    "    sample = mdata.obs['sample'][0]\n",
    "    if plot:\n",
    "        sb_plot =sb.jointplot(\n",
    "            data=mdata.mod['rna'].obs,\n",
    "            x=\"log_counts\",\n",
    "            y=\"log_genes\",\n",
    "            kind=\"hist\", bins=100, cmap=\"rocket_r\", color=\"#f69c73\", space=0\n",
    "        )\n",
    "        sb_plot.fig.suptitle(f'{sample}')\n",
    "        sb_plot.fig.tight_layout()\n",
    "        sb_plot.fig.subplots_adjust(top=0.95)\n",
    "        \n",
    "        #############################\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.scatter(x=mdata.mod['rna'].obs['n_counts_rank'], y=mdata.mod['rna'].obs['n_counts'], s=1, alpha=0.2, c='black', label='Total UMI Counts')\n",
    "        ax1.scatter(x=mdata.mod['rna'].obs['n_counts_rank'], y=mdata.mod['rna'].obs['n_genes'], s=1, alpha=0.2, c='tab:green', label='Gene Counts')\n",
    "        ax1.set(xscale='log', yscale='log')\n",
    "        ax1.set_ylabel('Total UMI/Gene Counts')\n",
    "        ax1.set_xlabel('Ranked Droplets')\n",
    "        ax1.set_title(sample)\n",
    "        #ax1.vlines(x=[max_rank], color=\"black\", lw=0.5).set_linestyle(\"--\")\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.scatter(x=mdata.mod['rna'].obs['n_counts_rank'], y=mdata.mod['rna'].obs['mt_frac']*100, s=1, alpha=0.2, c='tab:red', label='% Mito. Counts')\n",
    "        ax2.set_ylabel('%')\n",
    "        ax2.set_title(sample)\n",
    "\n",
    "        fig.legend(loc='center left', fontsize='xx-small', bbox_to_anchor=(0.2, 0.35))\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ###################################\n",
    "\n",
    "        cell_probs_key = 'log_cell_probs' #'log_cell_probs_' + mdata.mod['rna'].obs['sample'][0]\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.scatter(x=mdata.mod['rna'].obs['n_counts_rank'], y=mdata.mod['rna'].obs['n_counts'], s=1, alpha=0.2, c='black', label='Total UMI Counts')\n",
    "        ax1.scatter(x=mdata.mod['rna'].obs['n_counts_rank'], y=mdata.mod['rna'].obs['n_genes'], s=1, alpha=0.2, c='tab:green', label='Gene Counts')\n",
    "        ax1.set(xscale='log', yscale='log')\n",
    "        ax1.set_ylabel('Total UMI/Gene Counts')\n",
    "        ax1.set_xlabel('Ranked Droplets')\n",
    "        ax1.set_title(sample)\n",
    "        #ax1.vlines(x=[max_rank], color=\"black\", lw=0.5).set_linestyle(\"--\")\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.scatter(x=mdata.mod['rna'].obs['n_counts_rank'], y=mdata.mod['rna'].obs[cell_probs_key], s=1, alpha=0.2, c='tab:blue', label='Log Cell Probabilities')\n",
    "        ax2.set_ylabel('Cell Probabilities')\n",
    "        ax2.set_title(sample)\n",
    "\n",
    "        fig.legend(loc='center left', fontsize='xx-small', bbox_to_anchor=(0.2, 0.35))\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ##############################\n",
    "\n",
    "        sb.histplot(mdata.mod['rna'].obs[cell_probs_key][~np.isnan(list(mdata.mod['rna'].obs[cell_probs_key]))], kde=True, bins=60)\n",
    "        plt.title(label=f'Log Cell Probabilities of {sample}', fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ded3af",
   "metadata": {},
   "source": [
    "## plot settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb8056",
   "metadata": {},
   "source": [
    "#### plot titles\n",
    "sb.jointplot:\n",
    "\n",
    "sb.jointplot()\n",
    "plt.suptitle()\n",
    "\n",
    "umap:\n",
    "fig = sc.plp.umap(return_fig=True)\n",
    "ax = fig.axes[0]\n",
    "ax.legend_.set_title()\n",
    "\n",
    "histplot:\n",
    "sb.histplot()\n",
    "plt.title()\n",
    "\n",
    "scatterplot:\n",
    "p = sc.pl.scatter()\n",
    "plt.suptitle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bb1f1",
   "metadata": {},
   "source": [
    "### plot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "def load_RdOrYl_cmap_settings(fig_height=6, fig_width =6, dpi = 150, save_dpi =300, transparent = True, vector_fr=False,fr_on=True):\n",
    "    # Plot settings\n",
    "    \n",
    "\n",
    "    ## Plotting parameters\n",
    "    rcParams['figure.figsize']=(fig_height,fig_width) #rescale figures\n",
    "    #sc.set_figure_params(scanpy=True, frameon=False, vector_friendly=False, color_map='tab10' ,transparent=True, dpi=150, dpi_save=300)\n",
    "    sc.set_figure_params(scanpy=True, frameon=fr_on, vector_friendly=vector_fr ,transparent=transparent, dpi=dpi, dpi_save=save_dpi)\n",
    "\n",
    "    ## Grid & Ticks\n",
    "    rcParams['grid.alpha'] = 0\n",
    "    rcParams['xtick.bottom'] = True\n",
    "    rcParams['ytick.left'] = True\n",
    "\n",
    "    from matplotlib import colors\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"NewComputerModern10\", #Computer Modern Roman fontsize 10\n",
    "    })\n",
    "    ## Define new default settings\n",
    "    plt.rcParamsDefault = plt.rcParams\n",
    "\n",
    "    ## Embed font\n",
    "    plt.rc('pdf', fonttype=42)\n",
    "\n",
    "    ## Define new default settings\n",
    "    plt.rcParamsDefault = plt.rcParams\n",
    "\n",
    "    # Color maps\n",
    "    colors2 = plt.cm.YlOrRd(np.linspace(0.05, 1, 150)) \n",
    "    colors3 = plt.cm.Greys_r(np.linspace(0.8,0.9,1)) \n",
    "    colorsComb = np.vstack([colors3, colors2]) \n",
    "    mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb)\n",
    "    return mymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a520a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def load_YellowGnBl_cmap_settings(fig_height, fig_width):\n",
    "    # Plot settings\n",
    "\n",
    "    ## Plotting parameters\n",
    "    rcParams['figure.figsize']=(fig_height,fig_width) #rescale figures\n",
    "    #sc.set_figure_params(scanpy=True, frameon=False, vector_friendly=False, color_map='tab10' ,transparent=True, dpi=150, dpi_save=300)\n",
    "    sc.set_figure_params(scanpy=True, frameon=False, vector_friendly=False ,transparent=True, dpi=150, dpi_save=300)\n",
    "\n",
    "    ## Grid & Ticks\n",
    "    rcParams['grid.alpha'] = 0\n",
    "    rcParams['xtick.bottom'] = True\n",
    "    rcParams['ytick.left'] = True\n",
    "\n",
    "    from matplotlib import colors\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"NewComputerModern10\", #Computer Modern Roman fontsize 10\n",
    "    })\n",
    "    ## Define new default settings\n",
    "    plt.rcParamsDefault = plt.rcParams\n",
    "\n",
    "    ## Embed font\n",
    "    plt.rc('pdf', fonttype=42)\n",
    "\n",
    "    ## Define new default settings\n",
    "    plt.rcParamsDefault = plt.rcParams\n",
    "\n",
    "   # Color maps\n",
    "    colors2 = plt.cm.YlGnBu(np.linspace(0.05, 0.9, 150)) \n",
    "    colors3 = plt.cm.Greys_r(np.linspace(0.9,1,1)) \n",
    "    colorsComb = np.vstack([colors3, colors2]) \n",
    "    mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb)\n",
    "    return mymap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def load_BluePurple_cmap_settings(fig_height, fig_width):\n",
    "    # Plot settings\n",
    "\n",
    "    ## Plotting parameters\n",
    "    rcParams['figure.figsize']=(fig_height,fig_width) #rescale figures\n",
    "    #sc.set_figure_params(scanpy=True, frameon=False, vector_friendly=False, color_map='tab10' ,transparent=True, dpi=150, dpi_save=300)\n",
    "    sc.set_figure_params(scanpy=True, frameon=False, vector_friendly=False ,transparent=True, dpi=150, dpi_save=300)\n",
    "\n",
    "    ## Grid & Ticks\n",
    "    rcParams['grid.alpha'] = 0\n",
    "    rcParams['xtick.bottom'] = True\n",
    "    rcParams['ytick.left'] = True\n",
    "\n",
    "    from matplotlib import colors\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"NewComputerModern10\", #Computer Modern Roman fontsize 10\n",
    "    })\n",
    "    ## Define new default settings\n",
    "    plt.rcParamsDefault = plt.rcParams\n",
    "\n",
    "    ## Embed font\n",
    "    plt.rc('pdf', fonttype=42)\n",
    "\n",
    "    ## Define new default settings\n",
    "    plt.rcParamsDefault = plt.rcParams\n",
    "\n",
    "    # Color maps\n",
    "    colors2 = plt.cm.BuPu(np.linspace(0.05, 0.9, 150)) \n",
    "    colors3 = plt.cm.Greys_r(np.linspace(0.9,1,1)) \n",
    "    colorsComb = np.vstack([colors3, colors2]) \n",
    "    mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb)\n",
    "    return mymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding_density_kde(\n",
    "    adata,\n",
    "    groupby = 'batch',\n",
    "    basis = 'umap',\n",
    "    size = 10,\n",
    "    kde_levels = 10,\n",
    "    kde_linewidths = 0.5,\n",
    "    kde_bw_adjust = 0.75,\n",
    "    kde_thresh = 0.2,\n",
    "    n_cols = 3,  # specify the desired number of columns\n",
    "    save = False, cmap_kde = ''\n",
    "):\n",
    "    #ListedColormap(['#a8a8a8', '#939393', '#808080', '#6d6d6d', '#5a5a5a', '#484848', '#373737', '#262626', '#171717', '#000000'])\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    \n",
    "    # Compute densities\n",
    "    if not groupby+'_density' in adata.obs:\n",
    "        sc.tl.embedding_density(adata, basis=basis, groupby=groupby, key_added=groupby+'_density')\n",
    "\n",
    "    # Set the number of rows and columns for subplots\n",
    "    #n_rows = n_rows  # specify the desired number of rows\n",
    "    n_cols = n_cols  # specify the desired number of columns\n",
    "    n_rows = int(np.ceil(len(adata.obs[groupby].cat.categories)/n_cols))\n",
    "\n",
    "    # Calculate the total figure size for square subplots\n",
    "    figsize = (n_cols * 4, n_rows * 4)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)  # specify the figure size\n",
    "\n",
    "    # Flatten the axes array to a 1D array\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop over categories in groupby\n",
    "    X = 'X_' + basis\n",
    "    for i, sample in enumerate(adata.obs[groupby].cat.categories):\n",
    "        # Create scatter plot on the current subplot\n",
    "        ax = axes[i]\n",
    "        x = pd.DataFrame(adata.obsm[X])[0]\n",
    "        y = pd.DataFrame(adata.obsm[X])[1]\n",
    "        sb.scatterplot(x=x, y=y, color='#cccccc', s=size, linewidth=0, alpha=0.2, ax=ax)\n",
    "\n",
    "        # Create colored scatter & KDE plot on the current subplot\n",
    "        x = pd.DataFrame(adata[adata.obs[groupby].isin([sample])].obsm[X])[0]\n",
    "        y = pd.DataFrame(adata[adata.obs[groupby].isin([sample])].obsm[X])[1]\n",
    "        c = adata[adata.obs[groupby].isin([sample])].obs[groupby+'_density']\n",
    "        cmap = LinearSegmentedColormap.from_list(\"Custom\", ['#bebebe',adata.uns[groupby + '_colors'][i]], N=100)\n",
    "        sb.scatterplot(x=x, y=y, c=c, s=size, linewidth=0, alpha=0.5, cmap=cmap, ax=ax)\n",
    "        sb.kdeplot(x=x, y=y, levels=kde_levels, cmap=cmap_kde, linewidths=kde_linewidths, bw_adjust=kde_bw_adjust, thresh=kde_thresh, ax=ax)\n",
    "\n",
    "        # Customize plot appearance\n",
    "        ax.set_frame_on(False)\n",
    "        ax.set(xlabel='')\n",
    "        ax.set(xticklabels=[])\n",
    "        ax.set(ylabel='')\n",
    "        ax.set(yticklabels=[])\n",
    "        ax.tick_params(bottom=False, left=False)\n",
    "        ax.set_title(sample)\n",
    "\n",
    "    # Remove any extra empty subplots\n",
    "    for i in range(len(adata.obs[groupby].cat.categories), n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplot layout\n",
    "    if save:\n",
    "        plt.savefig(str(sc.settings.figdir) + \"/embedding-density-kde_\" + basis + \"_\" + groupby + \".pdf\")\n",
    "    plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_composition(adata, \n",
    "x_key=None, \n",
    "y_key=None, \n",
    "x_labels = None,\n",
    "y_labels = None,\n",
    "y_colors = None,\n",
    "title=None,                     \n",
    "width = 0.85,       # the width of the bars: can also be len(x) sequence\n",
    "x_rotation = 0,\n",
    "y_lim_offset = 2.5,\n",
    "x_lim_offset = 0.45,\n",
    "figsize= (6, 4)):\n",
    "    with rc_context({'figure.figsize': figsize}): #rcParams['figure.figsize']=(6,4)\n",
    "        if (x_labels == None):\n",
    "            x_labels = list(adata.obs[x_key].cat.categories)\n",
    "        \n",
    "        if (y_labels == None):\n",
    "            y_labels = list(adata.obs[y_key].cat.categories)\n",
    "        \n",
    "        if (y_colors == None):\n",
    "            y_colors = list(adata.uns[y_key + '_colors'])\n",
    "            \n",
    "        dic = {'x_labels':x_labels}\n",
    "        \n",
    "        for y_label in y_labels:\n",
    "            x_values = []\n",
    "            for x_label in x_labels:\n",
    "                x_value = adata.obs[y_key][adata.obs[x_key]==x_label].value_counts()[y_label]/adata.obs[y_key][adata.obs[x_key]==x_label].value_counts().sum()*100\n",
    "                x_values.append(x_value)\n",
    "            dic[y_label] = x_values\n",
    "        \n",
    "        df = pd.DataFrame(data = dic)\n",
    "\n",
    "        ax = df.plot(x='x_labels', kind='bar', stacked=True, width=width, edgecolor='0', linewidth=0.5, color=y_colors)\n",
    "\n",
    "        ax.set_ylabel('%')\n",
    "        ax.set_xlabel('')\n",
    "        if (title == None):\n",
    "            ax.set_title(y_key + ' by ' + x_key)\n",
    "        else:\n",
    "            ax.set_title(title)\n",
    "            \n",
    "        ax.axes.set_xticklabels(labels=x_labels, rotation=x_rotation)\n",
    "        ax.legend(bbox_to_anchor=(1, .5),loc='center left', edgecolor='1')\n",
    "\n",
    "        plt.ylim([-y_lim_offset,100+y_lim_offset])\n",
    "        plt.xlim([-1+x_lim_offset,len(x_labels)-x_lim_offset])\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf31454-46cf-416d-8925-9edcafccbc03",
   "metadata": {},
   "source": [
    "## gex QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87726ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_metrics(adata, ambient=True, plot=True, counts_per_gene=True, keep_dense=True):\n",
    "    \"\"\"\\\n",
    "    Calculate QC metrics.\n",
    "    genome: {'auto','Mus_musculus','Homo_sapiens','Sus_scrofa'}\n",
    "    mt_genes_path: Path to mitochondrial genes for sus scrofa. Tab delimited file without header and with gene symbols in column 2. default: '/mnt/ssd/Resources/sus_scrofa_mt_ens101_ext.txt'\n",
    "    ambient: Requires adata.var['is_ambient'] = pd.Categorical(list(map(str,list(adata.var['ambient_genes'] > cut_off))))\n",
    "    \"\"\"\n",
    "    \n",
    "    if sci.sparse.issparse(adata.X):\n",
    "        adata.X = adata.X.toarray()\n",
    "\n",
    "    if counts_per_gene:\n",
    "        # counts per gene\n",
    "        adata.var['n_counts'] = adata.X.sum(0)\n",
    "\n",
    "    # counts per cell\n",
    "    adata.obs['n_counts'] = adata.X.sum(1)\n",
    "    # log counts per cell\n",
    "    adata.obs['log_counts'] = np.log(adata.obs['n_counts'])\n",
    "    # rank by counts\n",
    "    adata.obs['n_counts_rank'] = adata.obs['n_counts'].rank(method='first',ascending=False)\n",
    "    # genes per cell\n",
    "    adata.obs['n_genes'] = (adata.X > 0).sum(1)\n",
    "    # log genes per cell\n",
    "    adata.obs['log_genes'] = np.log(adata.obs['n_genes'])\n",
    "    # fraction of mitochondrial genes\n",
    "    mt_gene_mask = [gene.startswith('mt-') for gene in adata.var_names]\n",
    "    adata.obs['mt_frac'] = adata.X[:, mt_gene_mask].sum(1)/adata.obs['n_counts']\n",
    "\n",
    "    rp_gene_mask = [gene.startswith(('Rps','Rpl')) for gene in adata.var_names]\n",
    "    adata.obs['rp_frac'] = adata.X[:,rp_gene_mask].sum(1) / adata.obs['n_counts']\n",
    "\n",
    "    if ambient:\n",
    "        adata.obs['ambi_frac'] = adata.X[:,adata.var['is_ambient']==True].sum(1) / adata.obs['n_counts']\n",
    "\n",
    "    if plot:\n",
    "        try:\n",
    "            sb.jointplot(\n",
    "                data=adata.obs,\n",
    "                x=\"log_counts\",\n",
    "                y=\"log_genes\",\n",
    "                kind=\"hist\", bins=100, cmap=\"rocket_r\", color=\"#f69c73\", space=0\n",
    "            )\n",
    "        except AttributeError:\n",
    "            sb.jointplot(\n",
    "                data=adata.obs,\n",
    "                x=\"log_counts\",\n",
    "                y=\"log_genes\",\n",
    "                kind=\"hist\", bins=100, space=0\n",
    "            )\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.scatter(x=adata.obs['n_counts_rank'], y=adata.obs['n_counts'], s=1, alpha=0.2, c='black', label='Total UMI Counts')\n",
    "        ax1.scatter(x=adata.obs['n_counts_rank'], y=adata.obs['n_genes'], s=1, alpha=0.2, c='tab:green', label='Gene Counts')\n",
    "        ax1.set(xscale='log', yscale='log')\n",
    "        ax1.set_ylabel('Total UMI/Gene Counts')\n",
    "        ax1.set_xlabel('Ranked Droplets')\n",
    "        #ax1.vlines(x=[max_rank], color=\"black\", lw=0.5).set_linestyle(\"--\")\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.scatter(x=adata.obs['n_counts_rank'], y=adata.obs['mt_frac']*100, s=1, alpha=0.2, c='tab:red', label='% Mito. Counts')\n",
    "        ax2.set_ylabel('%')\n",
    "\n",
    "        fig.legend(loc='center left', fontsize='xx-small', bbox_to_anchor=(0.2, 0.35))\n",
    "        \n",
    "    if not keep_dense:\n",
    "        adata.X = sci.sparse.csr_matrix(adata.X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04edaed-f3a1-451a-8b27-6fe7be7155b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_umap_leiden(adata, resolution=0.5, exclude_highly_expressed=False):\n",
    "       \n",
    "    # preprocess adata, cluster and get umap\n",
    "    adata_pp = adata.copy()\n",
    "    sc.pp.normalize_total(adata_pp, target_sum=1e4, exclude_highly_expressed=exclude_highly_expressed, key_added='size_factor') #sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6)\n",
    "    #sc.pp.log1p(adata_pp)\n",
    "    sc.pp.pca(adata_pp)\n",
    "    sc.pp.neighbors(adata_pp, metric='correlation')\n",
    "    sc.tl.leiden(adata_pp, resolution=resolution)\n",
    "    sc.tl.umap(adata_pp)\n",
    "    \n",
    "    adata.obsm['X_umap'] = adata_pp.obsm['X_umap'].copy()\n",
    "    adata.obs['leiden'] = adata_pp.obs['leiden'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80944148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_genes(adata,threshold = 20):\n",
    "    print(f\"Total number of genes: {adata.n_vars}\")\n",
    "\n",
    "    # Min 20 cells - filters out 0 count genes\n",
    "    sc.pp.filter_genes(adata, min_cells=threshold)\n",
    "    print(f\"Number of genes after cell filter: {adata.n_vars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_filter_mdata(mdata, adata, modality=None, qc_filter=None): \n",
    "    \"\"\"\\\n",
    "    mdata: mdata oject containing different modalities.\n",
    "    adata: adata object containing the modality to filter\n",
    "    modality: modality to filter\n",
    "    qc_filter: array of booleans. E.g. genes_filter = adata.obs['n_genes'] > min_genes or (genes_filter & counts_filter)\n",
    "    \"\"\"\n",
    "    if (modality is None):\n",
    "        print('Specify modality (\\'rna\\' or \\'atac\\'.')\n",
    "        return\n",
    "    if (qc_filter is None):\n",
    "        print('Specify QC filter.')\n",
    "        return\n",
    "    \n",
    "    pre_filter_n_obs = mdata.mod[modality].n_obs\n",
    "    mdata.mod[modality] = mdata.mod[modality][qc_filter]\n",
    "    adata = mdata.mod[modality]\n",
    "    pct = (pre_filter_n_obs - mdata.mod[modality].n_obs) / pre_filter_n_obs * 100\n",
    "    print('Filtered out {:d}'.format(pre_filter_n_obs - mdata.mod[modality].n_obs),'cells ({:.1f}'.format(pct) ,'%).')\n",
    "    print('Number of cells after filter: {:d}'.format(mdata.mod[modality].n_obs))\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065da442",
   "metadata": {},
   "source": [
    "## ATAC QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76788cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atac_qc_metrics(atac, mdata, n_tss=2000, plot=True, gtf_path=None):\n",
    "    \"\"\"\\\n",
    "    Calculate QC metrics.\n",
    "    atac: adata object containing atac data. i.e. mdata.mod['atac']\n",
    "    mdata: Corresponding mdata object\n",
    "    tss: {'random','top'} random tss or the n tss with highest rna counts\n",
    "    n_top: number of tss\n",
    "    \"\"\"\n",
    "    import muon as mu\n",
    "    from muon import atac as ac\n",
    "    \n",
    "    if (gtf_path is None):\n",
    "        print('Provide path to annotation GTF file!')\n",
    "        return\n",
    "    \n",
    "    if sci.sparse.issparse(atac.X):\n",
    "        print('X is spare. Densifying X...')\n",
    "        atac.X = atac.X.toarray()\n",
    "    \n",
    "    # counts per peak\n",
    "    atac.var['n_counts'] = atac.X.sum(0)\n",
    "    # counts per cell\n",
    "    atac.obs['n_counts_ATAC'] = list(atac.X.sum(1))\n",
    "    # log counts per cell\n",
    "    atac.obs['log_counts_ATAC'] = np.log(atac.obs['n_counts_ATAC'] + 1)\n",
    "    # rank by counts\n",
    "    atac.obs['n_counts_rank_ATAC'] = atac.obs['n_counts_ATAC'].rank(method='first',ascending=False)\n",
    "    # genes per cell\n",
    "    atac.obs['n_peaks_ATAC'] = (atac.X > 0).sum(1)\n",
    "    # log genes per cell\n",
    "    atac.obs['log_peaks_ATAC'] = np.log(atac.obs['n_peaks_ATAC'] + 1)\n",
    "    \n",
    "    if plot:\n",
    "        mu.pl.histogram(atac, ['n_counts_ATAC', 'n_peaks_ATAC'])\n",
    "        \n",
    "    # nucleosome signal\n",
    "    ac.tl.nucleosome_signal(atac, n=1e6)\n",
    "    \n",
    "    if plot:\n",
    "        # plot nucleosome signal\n",
    "        ac.pl.fragment_histogram(atac, region='1:1-20000000')\n",
    "        mu.pl.histogram(atac, \"nucleosome_signal\", kde=True)\n",
    "        \n",
    "    # TSS enrichment\n",
    "    features = get_top_feature_pos_from_gtf(mdata.mod['rna'], gtf_path=gtf_path, n_top=n_tss)\n",
    "    print(len(features))\n",
    "    tss = ac.tl.tss_enrichment(mdata, features=features, n_tss=n_tss)  # by default, features=ac.tl.get_gene_annotation_from_rna(mdata)\n",
    "    if plot:\n",
    "        # plot TSS enrichment\n",
    "        ac.pl.tss_enrichment(tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa74b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_pos_from_gtf(gtf_path=None, random=True):\n",
    "    # load Ensembl annotation file and reduce to genes\n",
    "    annotation = pd.read_csv(gtf_path, header=None, skiprows=5, sep='\\t')\n",
    "    annotation = annotation.loc[annotation.iloc[:,2]=='gene',:]\n",
    "\n",
    "    # get positions of + features\n",
    "    features_p = annotation.loc[annotation.iloc[:,6]=='+',:].iloc[:,[0,3,4]]     \n",
    "    features_p.columns = ['Chromosome','Start','End']\n",
    "\n",
    "    # get positions of - features and switch start and end\n",
    "    features_m = annotation.loc[annotation.iloc[:,6]=='-',:].iloc[:,[0,4,3]]     \n",
    "    features_m.columns = ['Chromosome','Start','End']\n",
    "\n",
    "    # concatenate\n",
    "    features = pd.concat([features_p, features_m], ignore_index = True)\n",
    "    \n",
    "    # randomize order\n",
    "    if random:\n",
    "        features = features.sample(frac=1, random_state=420)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_feature_pos_from_gtf(gex_adata, gtf_path=None, n_top=2000):\n",
    "    # load Ensembl annotation file and reduce to genes\n",
    "    annotation = pd.read_csv(gtf_path, header=None, skiprows=5, sep='\\t')\n",
    "    annotation = annotation.loc[annotation.iloc[:,2]=='gene',:]\n",
    "    \n",
    "    # add gene names\n",
    "    annotation.loc[:,9] = annotation.loc[:,8].str.split('\\\"', expand=True).loc[:,1]\n",
    "    annotation.set_index = annotation[9]\n",
    "   \n",
    "    # add counts and sort annotation\n",
    "    annotation = annotation.merge(gex_adata.var.loc[:,['gene_ids','n_counts']], left_on=9, right_on='gene_ids')\n",
    "    annotation = annotation.sort_values(by=['n_counts'], ascending=False)\n",
    "    \n",
    "    # filter n_top genes\n",
    "    annotation = annotation.iloc[0:n_top,:]\n",
    "      \n",
    "    # get positions of + features\n",
    "    features_p = annotation.loc[annotation.iloc[:,6]=='+',:].iloc[:,[0,3,4]]     \n",
    "    features_p.columns = ['Chromosome','Start','End']\n",
    "\n",
    "    # get positions of - features and switch start and end\n",
    "    features_m = annotation.loc[annotation.iloc[:,6]=='-',:].iloc[:,[0,4,3]]\n",
    "    features_m.columns = ['Chromosome','Start','End']\n",
    "\n",
    "    # concatenate\n",
    "    features = pd.concat([features_p, features_m], ignore_index = True)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signac_qc_metrics_list(mdata_filtered, \n",
    "                      cr_path=None,\n",
    "                      sample=None,\n",
    "                      aggregated=False,\n",
    "                      species='Hsapiens',\n",
    "                      genome='GRCh38',\n",
    "                      ensembl_release='v111',\n",
    "                      plot=True\n",
    "                     ):\n",
    "    \"\"\"\\\n",
    "    Calculate QC metrics.\n",
    "    atac: adata object containing atac data. i.e. mdata.mod['atac']\n",
    "    cr_path: Path to CellRanger results (cr_count or cr_agg)\n",
    "    sample: sample name\n",
    "    aggregated: Aggregated CellRanger Results. If True, sample has to be None\n",
    "    species='Hsapiens' or 'Mmusculus'\n",
    "    genome='GRCh38' or GRCm39 or hg38 or mm10\n",
    "    ensembl_release='v111'\n",
    "    plot: plot results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load packages and annotations\n",
    "    if genome not in ['GRCh38','GRCm38','hg19','hg38','mm10']:\n",
    "        print('Unkown genome. Genome has to be one of the following: GRCh38, GRCm38, hg19, hg38, mm10')\n",
    "        return None\n",
    "    \n",
    "    if genome.startswith('GRC'):\n",
    "        seqStyle = 'NCBI'\n",
    "    elif genome.startswith('hg') or genome.startswith('mm'):\n",
    "        seqStyle = 'UCSC'\n",
    "    \n",
    "    #sample = atac.obs['sample'][0]\n",
    "\n",
    "    print('Loading package BSgenome.' + species + '.' + seqStyle + '.' + genome)\n",
    "    ro.globalenv['bs_genome'] = 'BSgenome.' + species + '.' + seqStyle + '.' + genome\n",
    "    \n",
    "    print('Loading package EnsDb.' + species + '.' + ensembl_release)\n",
    "    ro.globalenv['ensdb'] = 'EnsDb.' + species + '.' + ensembl_release\n",
    "    \n",
    "    bl_dict = {'hg19':'blacklist_hg19',\n",
    "               'hg38':'blacklist_hg38_unified',\n",
    "               'GRCh38':'blacklist_hg38_unified',\n",
    "               'mm10':'blacklist_mm10',\n",
    "               'GRCm38':'blacklist_mm10'\n",
    "              }\n",
    "    \n",
    "    print('Using Blacklist: ' + bl_dict[genome])\n",
    "    ro.globalenv['blacklist_name'] = bl_dict[genome]\n",
    "    ro.globalenv['genome'] = genome\n",
    "    #workaround for blacklist to be renamed:\n",
    "    seqStyle = 'NCBI'\n",
    "    ro.globalenv['seqStyle'] = seqStyle\n",
    "    \n",
    "    \n",
    "    ro.r('''\n",
    "    library(Signac)\n",
    "    library(Seurat)\n",
    "\n",
    "    library(bs_genome,character.only=TRUE)\n",
    "    library(ensdb,character.only=TRUE)\n",
    "    library(stringr)\n",
    "\n",
    "    # extract gene annotations from EnsDb\n",
    "    annotations <- GetGRangesFromEnsDb(ensdb = get(ensdb))\n",
    "\n",
    "    # rename blacklist\n",
    "    blacklist = get(blacklist_name)\n",
    "    if(seqStyle == \"NCBI\"){\n",
    "        blacklist <- renameSeqlevels(blacklist, value = str_replace(str_replace(seqlevels(blacklist), pattern = \"chr\", replacement = \"\"), pattern = \"M\", replacement = \"MT\"))\n",
    "    }\n",
    "    ''')\n",
    "    for folder_name, mdata in mdata_filtered.items():  \n",
    "        sample = folder_name\n",
    "        print('analysing sample: ' + sample)\n",
    "        atac = mdata.mod['atac'] \n",
    "        \n",
    "        # Prepare data\n",
    "        if aggregated:\n",
    "            frag_path = cr_path + '/outs/atac_fragments.tsv.gz'\n",
    "            outs_path = cr_path + '/outs/'\n",
    "        else:\n",
    "            frag_path = cr_path + sample + '/outs/atac_fragments.tsv.gz'\n",
    "            outs_path = cr_path + sample + '/outs/'\n",
    "        \n",
    "        print(frag_path)\n",
    "            \n",
    "        ro.globalenv['count_mat'] = atac.X.T\n",
    "        ro.globalenv['obs_names'] = atac.obs_names\n",
    "        ro.globalenv['var_names'] = atac.var_names\n",
    "        ro.globalenv['obs'] = atac.obs\n",
    "        ro.globalenv['var'] = atac.var\n",
    "        ro.globalenv['fragments'] = frag_path\n",
    "        ro.globalenv['outs_path'] = outs_path\n",
    "        \n",
    "        # Generate Seurat object\n",
    "        ro.r('''\n",
    "        rownames(count_mat) <- var_names\n",
    "        colnames(count_mat) <- obs_names\n",
    "        chrom_assay <- CreateChromatinAssay(counts = count_mat, sep = c(\":\", \"-\"), fragments = fragments, annotation = annotations)\n",
    "        seurat <- CreateSeuratObject(counts = chrom_assay, assay = \"atac\")\n",
    "        ''')\n",
    "        \n",
    "        # Calculate QC metrics\n",
    "        ro.r('''\n",
    "        ## claculate FRiP\n",
    "        #frag_counts <- CountFragments(fragments = fragments)\n",
    "        #rownames(frag_counts) <- frag_counts$CB\n",
    "        #frag_counts$fraction_reads_in_peaks <- frag_counts$frequency_count / frag_counts$reads_count\n",
    "        #seurat <- AddMetaData(seurat, metadata = frag_counts[rownames(seurat@meta.data), -1, drop = FALSE])\n",
    "        \n",
    "        # claculate FRiP and fraction of mitochondrial reads\n",
    "        per_barcode_metrics <- read.csv(paste0(outs_path, \"per_barcode_metrics.csv\"))\n",
    "        rownames(per_barcode_metrics) <- per_barcode_metrics$barcode\n",
    "        per_barcode_metrics <- per_barcode_metrics[per_barcode_metrics$is_cell == 1,]\n",
    "        colnames(per_barcode_metrics) <- paste0(\"cr_\",colnames(per_barcode_metrics))\n",
    "        per_barcode_metrics$cr_fraction_fragments_in_peaks = per_barcode_metrics$cr_atac_peak_region_fragments / per_barcode_metrics$cr_atac_fragments\n",
    "        per_barcode_metrics$cr_fraction_tss_fragments = per_barcode_metrics$cr_atac_TSS_fragments / per_barcode_metrics$cr_atac_fragments\n",
    "        per_barcode_metrics$cr_fraction_reads_in_mito = per_barcode_metrics$cr_atac_mitochondrial_reads / per_barcode_metrics$cr_atac_raw_reads\n",
    "        seurat <- AddMetaData(seurat, metadata = per_barcode_metrics[rownames(seurat@meta.data),22:ncol(per_barcode_metrics)])\n",
    "        \n",
    "        # calculate fraction of cut-site (i.e. counts) in blacklist\n",
    "        seurat$fraction_counts_in_blacklist <- FractionCountsInRegion(\n",
    "        object = seurat,\n",
    "        assay = \"atac\",\n",
    "        regions = blacklist)\n",
    "        \n",
    "        # compute nucleosome signal score per cell\n",
    "        seurat <- NucleosomeSignal(object = seurat)\n",
    "\n",
    "        # compute TSS enrichment score per cell\n",
    "        seurat <- TSSEnrichment(object = seurat, fast = FALSE)\n",
    "        ''')\n",
    "        \n",
    "        # TO DO: Get fragment sizes\n",
    "        \n",
    "        # Get results\n",
    "        ro.r('''\n",
    "        meta_data <- seurat@meta.data\n",
    "        tss_matrix <- GetAssayData(object = seurat, assay = \"atac\", slot = \"positionEnrichment\")[[\"TSS\"]]\n",
    "        ''')\n",
    "        atac.uns['tss_matrix'] = ro.globalenv['tss_matrix']\n",
    "        atac_meta_data = ro.globalenv['meta_data']\n",
    "\n",
    "        try:\n",
    "            with (ro.default_converter + pandas2ri.converter).context():\n",
    "                atac_meta_data = ro.conversion.get_conversion().rpy2py(atac_meta_data)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "            \n",
    "        columns = ['nCount_atac', 'nFeature_atac', 'cr_atac_raw_reads',\n",
    "        'cr_atac_unmapped_reads', 'cr_atac_lowmapq', 'cr_atac_dup_reads',\n",
    "        'cr_atac_chimeric_reads', 'cr_atac_mitochondrial_reads',\n",
    "        'cr_atac_fragments', 'cr_atac_TSS_fragments',\n",
    "        'cr_atac_peak_region_fragments', 'cr_atac_peak_region_cutsites',\n",
    "        'cr_fraction_fragments_in_peaks', 'cr_fraction_reads_in_mito',\n",
    "        'fraction_counts_in_blacklist', 'nucleosome_signal',\n",
    "        'nucleosome_percentile', 'TSS.enrichment', 'TSS.percentile', 'cr_fraction_tss_fragments']\n",
    "\n",
    "        atac.obs.loc[:,columns] = atac_meta_data.loc[atac.obs_names,columns]\n",
    "        \n",
    "        atac.obs['log_nCount_atac'] = np.log(atac.obs['nCount_atac'])\n",
    "        atac.obs['log_nFeature_atac'] = np.log(atac.obs['nFeature_atac'])\n",
    "        \n",
    "        if plot:\n",
    "            \n",
    "            from matplotlib.colors import LinearSegmentedColormap\n",
    "            from itertools import chain\n",
    "\n",
    "            # Set the number of rows and columns for subplots\n",
    "            #n_rows = n_rows  # specify the desired number of rows\n",
    "            n_cols = 1 #n_cols  # specify the desired number of columns\n",
    "            n_rows = 1 #int(np.ceil(len(adata.obs[groupby].cat.categories)/n_cols))\n",
    "\n",
    "            # Calculate the total figure size for square subplots\n",
    "            figsize = (n_cols * 4, n_rows * 4)\n",
    "\n",
    "            # Create a figure with subplots\n",
    "            fig, ax = plt.subplots(n_rows, n_cols, figsize=figsize)  # specify the figure size\n",
    "\n",
    "            # Create colored scatter & KDE plot on the current subplot\n",
    "            x = list(range(-1000,1001,1))\n",
    "            y = list(chain.from_iterable(atac.uns['tss_matrix'].mean(axis=0).A))\n",
    "            sb.lineplot(x=x, y=y, c='black', linewidth=0.5, alpha=1, ax=ax)\n",
    "\n",
    "            # Customize plot appearance\n",
    "            ax.set(xlabel='Distance to TSS')\n",
    "            ax.set(ylabel='Mean TSS Enrichment Score')\n",
    "            ax.set_title(f'TSS Enrichnemt {sample}')\n",
    "\n",
    "            plt.tight_layout()  # Adjust subplot layout\n",
    "\n",
    "            plt.show()  # Display the plot\n",
    "            plt.close()\n",
    "            \n",
    "            ################################################################\n",
    "            \n",
    "            keys=['log_nCount_atac', \n",
    "            'cr_fraction_fragments_in_peaks', 'cr_fraction_reads_in_mito',\n",
    "            'fraction_counts_in_blacklist', 'nucleosome_signal',\n",
    "            'TSS.enrichment']\n",
    "\n",
    "            n_cols = 6\n",
    "\n",
    "            # Set the number of rows and keys for subplots\n",
    "            #n_rows = n_rows  # specify the desired number of rows\n",
    "            n_cols = n_cols  # specify the desired number of keys\n",
    "            n_rows = int(np.ceil(len(keys)/n_cols))\n",
    "\n",
    "            # Calculate the total figure size for square subplots\n",
    "            figsize = (n_cols * 2.5, n_rows * 4)\n",
    "\n",
    "            # Create a figure with subplots\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)  # specify the figure size\n",
    "\n",
    "            # Flatten the axes array to a 1D array\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # Loop over categories in groupby\n",
    "            for i, key in enumerate(keys):\n",
    "                # Create scatter plot on the current subplot\n",
    "                ax = axes[i]\n",
    "                y = atac.obs[key]\n",
    "                sb.violinplot(y=y, color='#f69c73', inner='quart', linewidth=1, linecolor='black', ax=ax)\n",
    "                sb.stripplot(y=y, color='black', size=0.75, alpha=0.5, jitter=0.25,  ax=ax)\n",
    "\n",
    "                ## Customize plot appearance\n",
    "                ax.set(xlabel='')\n",
    "                ax.set(xticklabels=[])\n",
    "                ax.tick_params(bottom=False)\n",
    "\n",
    "            # Remove any extra empty subplots\n",
    "            for i in range(len(keys), n_rows * n_cols):\n",
    "                fig.delaxes(axes[i])\n",
    "\n",
    "            plt.tight_layout()  # Adjust subplot layout\n",
    "\n",
    "            plt.show()  # Display the plot\n",
    "            plt.close()\n",
    "            \n",
    "            #############################################################\n",
    "            \n",
    "            p = sb.jointplot(x=atac.obs['log_nCount_atac'], y=atac.obs['TSS.enrichment'], n_levels=15, thresh=0.05, kind=\"kde\", space=0, fill=True, cmap=\"rocket_r\", color=\"#f69c73\")\n",
    "            p.plot_joint(sb.scatterplot, c='#cccccc', s=10, linewidth=0, alpha=0.5)\n",
    "            p.plot_joint(sb.kdeplot, levels=15, fill=True, cmap='rocket_r', thresh=0.05, alpha=0.75)\n",
    "            p.plot_joint(sb.kdeplot, levels=15, fill=False, c='black', thresh=0.05, alpha=0.5, linewidths=0.25)\n",
    "            plt.suptitle(sample)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            #############################################################\n",
    "            \n",
    "            p = sb.jointplot(x=atac.obs['log_nCount_atac'], y=atac.obs['nucleosome_signal'], n_levels=15, thresh=0.05, kind=\"kde\", space=0, fill=True, cmap=\"rocket_r\", color=\"#f69c73\")\n",
    "            p.plot_joint(sb.scatterplot, c='#cccccc', s=10, linewidth=0, alpha=0.5)\n",
    "            p.plot_joint(sb.kdeplot, levels=15, fill=True, cmap='rocket_r', thresh=0.05, alpha=0.75)\n",
    "            p.plot_joint(sb.kdeplot, levels=15, fill=False, c='black', thresh=0.05, alpha=0.5, linewidths=0.25)\n",
    "            plt.suptitle(sample)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            #############################################################\n",
    "            \n",
    "            p = sb.jointplot(x=atac.obs['TSS.enrichment'], y=atac.obs['nucleosome_signal'], n_levels=15, thresh=0.05, kind=\"kde\", space=0, fill=True, cmap=\"rocket_r\", color=\"#f69c73\")\n",
    "            p.plot_joint(sb.scatterplot, c='#cccccc', s=10, linewidth=0, alpha=0.5)\n",
    "            p.plot_joint(sb.kdeplot, levels=15, fill=True, cmap='rocket_r', thresh=0.05, alpha=0.75)\n",
    "            p.plot_joint(sb.kdeplot, levels=15, fill=False, c='black', thresh=0.05, alpha=0.5, linewidths=0.25)\n",
    "            plt.suptitle(sample)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    return mdata_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signac_qc_metrics(atac, \n",
    "                      cr_path=None,\n",
    "                      sample=None,\n",
    "                      aggregated=False,\n",
    "                      species='Hsapiens',\n",
    "                      genome='GRCh38',\n",
    "                      ensembl_release='v111',\n",
    "                      plot=True\n",
    "                     ):\n",
    "    \"\"\"\\\n",
    "    Calculate QC metrics.\n",
    "    atac: adata object containing atac data. i.e. mdata.mod['atac']\n",
    "    cr_path: Path to CellRanger results (cr_count or cr_agg)\n",
    "    sample: sample name\n",
    "    aggregated: Aggregated CellRanger Results. If True, sample has to be None\n",
    "    species='Hsapiens' or 'Mmusculus'\n",
    "    genome='GRCh38' or GRCm39 or hg38 or mm10\n",
    "    ensembl_release='v111'\n",
    "    plot: plot results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load packages and annotations\n",
    "    if genome not in ['GRCh38','GRCm38','hg19','hg38','mm10']:\n",
    "        print('Unkown genome. Genome has to be one of the following: GRCh38, GRCm38, hg19, hg38, mm10')\n",
    "        return None\n",
    "    \n",
    "    if genome.startswith('GRC'):\n",
    "        seqStyle = 'NCBI'\n",
    "    elif genome.startswith('hg') or genome.startswith('mm'):\n",
    "        seqStyle = 'UCSC'\n",
    "    \n",
    "    #sample = atac.obs['sample'][0]\n",
    "    print('analysing sample: ' + sample)\n",
    "    print('Loading package BSgenome.' + species + '.' + seqStyle + '.' + genome)\n",
    "    ro.globalenv['bs_genome'] = 'BSgenome.' + species + '.' + seqStyle + '.' + genome\n",
    "    \n",
    "    print('Loading package EnsDb.' + species + '.' + ensembl_release)\n",
    "    ro.globalenv['ensdb'] = 'EnsDb.' + species + '.' + ensembl_release\n",
    "    \n",
    "    bl_dict = {'hg19':'blacklist_hg19',\n",
    "               'hg38':'blacklist_hg38_unified',\n",
    "               'GRCh38':'blacklist_hg38_unified',\n",
    "               'mm10':'blacklist_mm10',\n",
    "               'GRCm38':'blacklist_mm10'\n",
    "              }\n",
    "    \n",
    "    print('Using Blacklist: ' + bl_dict[genome])\n",
    "    ro.globalenv['blacklist_name'] = bl_dict[genome]\n",
    "    ro.globalenv['genome'] = genome\n",
    "    #workaround for blacklist to be renamed:\n",
    "    seqStyle = 'NCBI'\n",
    "    ro.globalenv['seqStyle'] = seqStyle\n",
    "    \n",
    "    \n",
    "    ro.r('''\n",
    "    library(Signac)\n",
    "    library(Seurat)\n",
    "\n",
    "    library(bs_genome,character.only=TRUE)\n",
    "    library(ensdb,character.only=TRUE)\n",
    "    library(stringr)\n",
    "\n",
    "    # extract gene annotations from EnsDb\n",
    "    annotations <- GetGRangesFromEnsDb(ensdb = get(ensdb))\n",
    "\n",
    "    # rename blacklist\n",
    "    blacklist = get(blacklist_name)\n",
    "    if(seqStyle == \"NCBI\"){\n",
    "        blacklist <- renameSeqlevels(blacklist, value = str_replace(str_replace(seqlevels(blacklist), pattern = \"chr\", replacement = \"\"), pattern = \"M\", replacement = \"MT\"))\n",
    "    }\n",
    "    ''')\n",
    "    \n",
    "    # Prepare data\n",
    "    if aggregated:\n",
    "        frag_path = cr_path + '/outs/atac_fragments.tsv.gz'\n",
    "        outs_path = cr_path + '/outs/'\n",
    "    else:\n",
    "        frag_path = cr_path + sample + '/outs/atac_fragments.tsv.gz'\n",
    "        outs_path = cr_path + sample + '/outs/'\n",
    "    \n",
    "    print(frag_path)\n",
    "        \n",
    "    ro.globalenv['count_mat'] = atac.X.T\n",
    "    ro.globalenv['obs_names'] = atac.obs_names\n",
    "    ro.globalenv['var_names'] = atac.var_names\n",
    "    ro.globalenv['obs'] = atac.obs\n",
    "    ro.globalenv['var'] = atac.var\n",
    "    ro.globalenv['fragments'] = frag_path\n",
    "    ro.globalenv['outs_path'] = outs_path\n",
    "    \n",
    "    # Generate Seurat object\n",
    "    ro.r('''\n",
    "    rownames(count_mat) <- var_names\n",
    "    colnames(count_mat) <- obs_names\n",
    "    chrom_assay <- CreateChromatinAssay(counts = count_mat, sep = c(\":\", \"-\"), fragments = fragments, annotation = annotations)\n",
    "    seurat <- CreateSeuratObject(counts = chrom_assay, assay = \"atac\")\n",
    "    ''')\n",
    "    \n",
    "    # Calculate QC metrics\n",
    "    ro.r('''\n",
    "    ## claculate FRiP\n",
    "    #frag_counts <- CountFragments(fragments = fragments)\n",
    "    #rownames(frag_counts) <- frag_counts$CB\n",
    "    #frag_counts$fraction_reads_in_peaks <- frag_counts$frequency_count / frag_counts$reads_count\n",
    "    #seurat <- AddMetaData(seurat, metadata = frag_counts[rownames(seurat@meta.data), -1, drop = FALSE])\n",
    "    \n",
    "    # claculate FRiP and fraction of mitochondrial reads\n",
    "    per_barcode_metrics <- read.csv(paste0(outs_path, \"per_barcode_metrics.csv\"))\n",
    "    rownames(per_barcode_metrics) <- per_barcode_metrics$barcode\n",
    "    per_barcode_metrics <- per_barcode_metrics[per_barcode_metrics$is_cell == 1,]\n",
    "    colnames(per_barcode_metrics) <- paste0(\"cr_\",colnames(per_barcode_metrics))\n",
    "    per_barcode_metrics$cr_fraction_fragments_in_peaks = per_barcode_metrics$cr_atac_peak_region_fragments / per_barcode_metrics$cr_atac_fragments\n",
    "    per_barcode_metrics$cr_fraction_tss_fragments = per_barcode_metrics$cr_atac_TSS_fragments / per_barcode_metrics$cr_atac_fragments\n",
    "    per_barcode_metrics$cr_fraction_reads_in_mito = per_barcode_metrics$cr_atac_mitochondrial_reads / per_barcode_metrics$cr_atac_raw_reads\n",
    "    seurat <- AddMetaData(seurat, metadata = per_barcode_metrics[rownames(seurat@meta.data),22:ncol(per_barcode_metrics)])\n",
    "    \n",
    "    # calculate fraction of cut-site (i.e. counts) in blacklist\n",
    "    seurat$fraction_counts_in_blacklist <- FractionCountsInRegion(\n",
    "    object = seurat,\n",
    "    assay = \"atac\",\n",
    "    regions = blacklist)\n",
    "    \n",
    "    # compute nucleosome signal score per cell\n",
    "    seurat <- NucleosomeSignal(object = seurat)\n",
    "\n",
    "    # compute TSS enrichment score per cell\n",
    "    seurat <- TSSEnrichment(object = seurat, fast = FALSE)\n",
    "    ''')\n",
    "    \n",
    "    # TO DO: Get fragment sizes\n",
    "    \n",
    "    # Get results\n",
    "    ro.r('''\n",
    "    meta_data <- seurat@meta.data\n",
    "    tss_matrix <- GetAssayData(object = seurat, assay = \"atac\", slot = \"positionEnrichment\")[[\"TSS\"]]\n",
    "    ''')\n",
    "    atac.uns['tss_matrix'] = ro.globalenv['tss_matrix']\n",
    "    atac_meta_data = ro.globalenv['meta_data']\n",
    "\n",
    "    try:\n",
    "        with (ro.default_converter + pandas2ri.converter).context():\n",
    "            atac_meta_data = ro.conversion.get_conversion().rpy2py(atac_meta_data)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "        \n",
    "    columns = ['nCount_atac', 'nFeature_atac', 'cr_atac_raw_reads',\n",
    "       'cr_atac_unmapped_reads', 'cr_atac_lowmapq', 'cr_atac_dup_reads',\n",
    "       'cr_atac_chimeric_reads', 'cr_atac_mitochondrial_reads',\n",
    "       'cr_atac_fragments', 'cr_atac_TSS_fragments',\n",
    "       'cr_atac_peak_region_fragments', 'cr_atac_peak_region_cutsites',\n",
    "       'cr_fraction_fragments_in_peaks', 'cr_fraction_reads_in_mito',\n",
    "       'fraction_counts_in_blacklist', 'nucleosome_signal',\n",
    "       'nucleosome_percentile', 'TSS.enrichment', 'TSS.percentile', 'cr_fraction_tss_fragments']\n",
    "\n",
    "    atac.obs.loc[:,columns] = atac_meta_data.loc[atac.obs_names,columns]\n",
    "    \n",
    "    atac.obs['log_nCount_atac'] = np.log(atac.obs['nCount_atac'])\n",
    "    atac.obs['log_nFeature_atac'] = np.log(atac.obs['nFeature_atac'])\n",
    "    \n",
    "    if plot:\n",
    "        \n",
    "        from matplotlib.colors import LinearSegmentedColormap\n",
    "        from itertools import chain\n",
    "\n",
    "        # Set the number of rows and columns for subplots\n",
    "        #n_rows = n_rows  # specify the desired number of rows\n",
    "        n_cols = 1 #n_cols  # specify the desired number of columns\n",
    "        n_rows = 1 #int(np.ceil(len(adata.obs[groupby].cat.categories)/n_cols))\n",
    "\n",
    "        # Calculate the total figure size for square subplots\n",
    "        figsize = (n_cols * 4, n_rows * 4)\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=figsize)  # specify the figure size\n",
    "\n",
    "        # Create colored scatter & KDE plot on the current subplot\n",
    "        x = list(range(-1000,1001,1))\n",
    "        y = list(chain.from_iterable(atac.uns['tss_matrix'].mean(axis=0).A))\n",
    "        sb.lineplot(x=x, y=y, c='black', linewidth=0.5, alpha=1, ax=ax)\n",
    "\n",
    "        # Customize plot appearance\n",
    "        ax.set(xlabel='Distance to TSS')\n",
    "        ax.set(ylabel='Mean TSS Enrichment Score')\n",
    "        ax.set_title(f'TSS Enrichnemt {sample}')\n",
    "\n",
    "        plt.tight_layout()  # Adjust subplot layout\n",
    "\n",
    "        plt.show()  # Display the plot\n",
    "        plt.close()\n",
    "        \n",
    "        ################################################################\n",
    "        \n",
    "        keys=['log_nCount_atac', \n",
    "       'cr_fraction_fragments_in_peaks', 'cr_fraction_reads_in_mito',\n",
    "       'fraction_counts_in_blacklist', 'nucleosome_signal',\n",
    "       'TSS.enrichment']\n",
    "\n",
    "        n_cols = 6\n",
    "\n",
    "        # Set the number of rows and keys for subplots\n",
    "        #n_rows = n_rows  # specify the desired number of rows\n",
    "        n_cols = n_cols  # specify the desired number of keys\n",
    "        n_rows = int(np.ceil(len(keys)/n_cols))\n",
    "\n",
    "        # Calculate the total figure size for square subplots\n",
    "        figsize = (n_cols * 2.5, n_rows * 4)\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)  # specify the figure size\n",
    "\n",
    "        # Flatten the axes array to a 1D array\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Loop over categories in groupby\n",
    "        for i, key in enumerate(keys):\n",
    "            # Create scatter plot on the current subplot\n",
    "            ax = axes[i]\n",
    "            y = atac.obs[key]\n",
    "            sb.violinplot(y=y, color='#f69c73', inner='quart', linewidth=1, linecolor='black', ax=ax)\n",
    "            sb.stripplot(y=y, color='black', size=0.75, alpha=0.5, jitter=0.25,  ax=ax)\n",
    "\n",
    "            ## Customize plot appearance\n",
    "            ax.set(xlabel='')\n",
    "            ax.set(xticklabels=[])\n",
    "            ax.tick_params(bottom=False)\n",
    "\n",
    "        # Remove any extra empty subplots\n",
    "        for i in range(len(keys), n_rows * n_cols):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        plt.tight_layout()  # Adjust subplot layout\n",
    "\n",
    "        plt.show()  # Display the plot\n",
    "        plt.close()\n",
    "        \n",
    "        #############################################################\n",
    "        \n",
    "        p = sb.jointplot(x=atac.obs['log_nCount_atac'], y=atac.obs['TSS.enrichment'], n_levels=15, thresh=0.05, kind=\"kde\", space=0, fill=True, cmap=\"rocket_r\", color=\"#f69c73\")\n",
    "        p.plot_joint(sb.scatterplot, c='#cccccc', s=10, linewidth=0, alpha=0.5)\n",
    "        p.plot_joint(sb.kdeplot, levels=15, fill=True, cmap='rocket_r', thresh=0.05, alpha=0.75)\n",
    "        p.plot_joint(sb.kdeplot, levels=15, fill=False, c='black', thresh=0.05, alpha=0.5, linewidths=0.25)\n",
    "        plt.suptitle(sample)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        #############################################################\n",
    "        \n",
    "        p = sb.jointplot(x=atac.obs['log_nCount_atac'], y=atac.obs['nucleosome_signal'], n_levels=15, thresh=0.05, kind=\"kde\", space=0, fill=True, cmap=\"rocket_r\", color=\"#f69c73\")\n",
    "        p.plot_joint(sb.scatterplot, c='#cccccc', s=10, linewidth=0, alpha=0.5)\n",
    "        p.plot_joint(sb.kdeplot, levels=15, fill=True, cmap='rocket_r', thresh=0.05, alpha=0.75)\n",
    "        p.plot_joint(sb.kdeplot, levels=15, fill=False, c='black', thresh=0.05, alpha=0.5, linewidths=0.25)\n",
    "        plt.suptitle(sample)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        #############################################################\n",
    "        \n",
    "        p = sb.jointplot(x=atac.obs['TSS.enrichment'], y=atac.obs['nucleosome_signal'], n_levels=15, thresh=0.05, kind=\"kde\", space=0, fill=True, cmap=\"rocket_r\", color=\"#f69c73\")\n",
    "        p.plot_joint(sb.scatterplot, c='#cccccc', s=10, linewidth=0, alpha=0.5)\n",
    "        p.plot_joint(sb.kdeplot, levels=15, fill=True, cmap='rocket_r', thresh=0.05, alpha=0.75)\n",
    "        p.plot_joint(sb.kdeplot, levels=15, fill=False, c='black', thresh=0.05, alpha=0.5, linewidths=0.25)\n",
    "        plt.suptitle(sample)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    return atac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4665fe",
   "metadata": {},
   "source": [
    "## general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fadea",
   "metadata": {},
   "source": [
    "### spasify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae884b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_mdata(mdata, modalities='all'):\n",
    "   \"\"\"\n",
    "   Loop trough all modalities and make dense adata.X sparse.\n",
    "   modalites: 'all' or list of modalities, e.g. ['rna','atac']\n",
    "   \"\"\"\n",
    "   if modalities=='all':\n",
    "       modalities = mdata.mod.keys()\n",
    "       \n",
    "   for mod in modalities:\n",
    "       if not sci.sparse.issparse(mdata.mod[mod].X):\n",
    "           density = sum(np.count_nonzero(mdata.mod[mod].X, axis=0))/(mdata.mod[mod].X.shape[0]*mdata.mod[mod].X.shape[1])\n",
    "           if density < 2/3:\n",
    "               print('Sparsify modality', mod)\n",
    "               mdata.mod[mod].X = sci.sparse.csr_matrix(mdata.mod[mod].X)\n",
    "           else:\n",
    "               print('Modality', mod, 'is stored dense. Density is ', density*100, ' %.')\n",
    "       else:\n",
    "           print('Modality', mod, 'already sparse...')\n",
    "           \n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "   \n",
    "   \n",
    "   \n",
    "def sparsify_all_layers(adata):\n",
    "   \"\"\"\n",
    "   Loop trough all layers and make dense matrices sparse.\n",
    "   \"\"\"\n",
    "         \n",
    "   if not sci.sparse.issparse(adata.X):\n",
    "       density = sum(np.count_nonzero(adata.X, axis=0))/(adata.shape[0]*adata.X.shape[1])\n",
    "       if density < 2/3:\n",
    "           print('Sparsify .X...')\n",
    "           adata.X = sci.sparse.csr_matrix(adata.X)\n",
    "       else:\n",
    "           print('.X is stored dense. Density is ', density*100, ' %.')\n",
    "   else:\n",
    "       print('.X already spase...')  \n",
    "       \n",
    "   for layer in list(adata.layers):\n",
    "       if not sci.sparse.issparse(adata.layers[layer]):\n",
    "           density = sum(np.count_nonzero(adata.layers[layer], axis=0))/(adata.shape[0]*adata.X.shape[1])\n",
    "           if density < 2/3:\n",
    "               print('Sparsify ', layer)\n",
    "               adata.layers[layer] = sci.sparse.csr_matrix(adata.layers[layer])\n",
    "           else:\n",
    "               print('Layer', layer,' is stored dense. Density is ', density*100, ' %.')\n",
    "       else:\n",
    "           print('Layer', layer, 'already sparse...')\n",
    "           \n",
    "           \n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "   \n",
    "\n",
    "def sparsify_all_layers_mdata(mdata, modalities='all'):\n",
    "   \"\"\"\n",
    "   Loop trough all modalities and make dense adata.X sparse.\n",
    "   modalites: 'all' or list of modalities, e.g. ['rna','atac']\n",
    "   \"\"\"\n",
    "   if modalities=='all':\n",
    "       modalities = mdata.mod.keys()\n",
    "       \n",
    "   for mod in modalities:\n",
    "       if not sci.sparse.issparse(mdata.mod[mod].X):\n",
    "           density = sum(np.count_nonzero(mdata.mod[mod].X, axis=0))/(mdata.mod[mod].X.shape[0]*mdata.mod[mod].X.shape[1])\n",
    "           if density < 2/3:\n",
    "               print('Sparsify .X in modality', mod)\n",
    "               mdata.mod[mod].X = sci.sparse.csr_matrix(mdata.mod[mod].X)\n",
    "           else:\n",
    "               print('Modality', mod, 'is stored dense. Density is ', density*100, ' %.')\n",
    "       else:\n",
    "           print('.X in modality', mod, 'already sparse...')\n",
    "           \n",
    "       for layer in list(mdata.mod[mod].layers):\n",
    "           if not sci.sparse.issparse(mdata.mod[mod].layers[layer]):\n",
    "               density = sum(np.count_nonzero(mdata.mod[mod].layers[layer], axis=0))/(mdata.mod[mod].layers[layer].shape[0]*mdata.mod[mod].layers[layer].shape[1])\n",
    "               if density < 2/3:\n",
    "                   print('Sparsify ', layer, ' in modality', mod)\n",
    "                   mdata.mod[mod].layers[layer] = sci.sparse.csr_matrix(mdata.mod[mod].layers[layer])\n",
    "               else:\n",
    "                   print('Layer', layer, ' in modality', mod, 'already sprase...')\n",
    "           else:\n",
    "               print('Layer', layer, ' in modality', mod, ' is stored dense. Density is ', density*100, ' %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402a221",
   "metadata": {},
   "source": [
    "## Doublet detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ebbc9",
   "metadata": {},
   "source": [
    "#### def threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(\n",
    "    clf,\n",
    "    show=False,\n",
    "    save=None,\n",
    "    log10=True,\n",
    "    log_p_grid=None,\n",
    "    voter_grid=None,\n",
    "    v_step=2,\n",
    "    p_step=5,\n",
    "):\n",
    "    \"\"\"Produce a plot showing number of cells called doublet across\n",
    "       various thresholds\n",
    "    Args:\n",
    "        clf (BoostClassifier object): Fitted classifier\n",
    "        show (bool, optional): If True, runs plt.show()\n",
    "        save (str, optional): If provided, the figure is saved to this\n",
    "            filepath.\n",
    "        log10 (bool, optional): Use log 10 if true, natural log if false.\n",
    "        log_p_grid (ndarray, optional): log p-value thresholds to use.\n",
    "            Defaults to np.arange(-100, -1). log base decided by log10\n",
    "        voter_grid (ndarray, optional): Voting thresholds to use. Defaults to\n",
    "            np.arange(0.3, 1.0, 0.05).\n",
    "        p_step (int, optional): number of xlabels to skip in plot\n",
    "        v_step (int, optional): number of ylabels to skip in plot\n",
    "    Returns:\n",
    "        matplotlib figure\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    # Ignore numpy complaining about np.nan comparisons\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        all_log_p_values_ = np.copy(clf.all_log_p_values_)\n",
    "        if log10:\n",
    "            all_log_p_values_ /= np.log(10)\n",
    "        if log_p_grid is None:\n",
    "            log_p_grid = np.arange(-100, -1)\n",
    "        if voter_grid is None:\n",
    "            voter_grid = np.arange(0.3, 1.0, 0.05)\n",
    "        doubs_per_t = np.zeros((len(voter_grid), len(log_p_grid)))\n",
    "        for i in range(len(voter_grid)):\n",
    "            for j in range(len(log_p_grid)):\n",
    "                voting_average = np.mean(\n",
    "                    np.ma.masked_invalid(all_log_p_values_) <= log_p_grid[j], axis=0\n",
    "                )\n",
    "                labels = np.ma.filled((voting_average >= voter_grid[i]).astype(float), np.nan)\n",
    "                doubs_per_t[i, j] = np.nansum(labels)\n",
    "\n",
    "    # Ignore warning for convergence plot\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action=\"ignore\", module=\"matplotlib\", message=\"^tight_layout\")\n",
    "\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=150)\n",
    "        cax = ax.imshow(doubs_per_t, cmap=\"turbo\", aspect=\"auto\")\n",
    "        ax.set_xticks(np.arange(len(log_p_grid))[::p_step])\n",
    "        ax.set_xticklabels(np.around(log_p_grid, 1)[::p_step], rotation=\"vertical\")\n",
    "        ax.set_yticks(np.arange(len(voter_grid))[::v_step])\n",
    "        ax.set_yticklabels(np.around(voter_grid, 2)[::v_step])\n",
    "        cbar = f.colorbar(cax)\n",
    "        cbar.set_label(\"Predicted Doublets\")\n",
    "        if log10 is True:\n",
    "            ax.set_xlabel(\"Log10 p-value\")\n",
    "        else:\n",
    "            ax.set_xlabel(\"Log p-value\")\n",
    "        ax.set_ylabel(\"Voting Threshold\")\n",
    "        ax.set_title(\"Threshold Diagnostics\")\n",
    "\n",
    "    if show is True:\n",
    "        plt.show()\n",
    "    if save:\n",
    "        f.savefig(save, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b8f0e",
   "metadata": {},
   "source": [
    "#### def umap_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dda5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_plot(\n",
    "    raw_counts,\n",
    "    labels,\n",
    "    n_components=30,\n",
    "    show=False,\n",
    "    save=None,\n",
    "    normalizer= None,\n",
    "    random_state=None,\n",
    "):\n",
    "    \"\"\"Produce a umap plot of the data with doublets in black.\n",
    "\n",
    "        Count matrix is normalized and dimension reduced before plotting.\n",
    "\n",
    "    Args:\n",
    "        raw_counts (array-like): Count matrix, oriented cells by genes.\n",
    "        labels (ndarray): predicted doublets from predict method\n",
    "        n_components (int, optional): number of PCs to use prior to UMAP\n",
    "        show (bool, optional): If True, runs plt.show()\n",
    "        save (str, optional): filename for saved figure,\n",
    "            figure not saved by default\n",
    "        normalizer ((ndarray) -> ndarray, optional): Method to normalize\n",
    "            raw_counts. Defaults to normalize_counts, included in this package.\n",
    "            Note: To use normalize_counts with its pseudocount parameter changed\n",
    "            from the default 0.1 value to some positive float `new_var`, use:\n",
    "            normalizer=lambda counts: doubletdetection.normalize_counts(counts,\n",
    "            pseudocount=new_var)\n",
    "        random_state (int, optional): If provided, passed to PCA and UMAP\n",
    "\n",
    "    Returns:\n",
    "        matplotlib figure\n",
    "        ndarray: umap reduction\n",
    "    \"\"\"\n",
    "    import doubletdetection\n",
    "    import os\n",
    "    import warnings\n",
    "\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    import umap\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.utils import check_array\n",
    "    try:\n",
    "        raw_counts = check_array(\n",
    "            raw_counts, accept_sparse=False, force_all_finite=True, ensure_2d=True\n",
    "        )\n",
    "    except TypeError:  # Only catches sparse error. Non-finite & n_dims still raised.\n",
    "        warnings.warn(\"Sparse raw_counts is automatically densified.\")\n",
    "        raw_counts = raw_counts.toarray()\n",
    "    norm_counts = doubletdetection.plot.normalize_counts(raw_counts)\n",
    "    reduced_counts = PCA(\n",
    "        n_components=n_components, svd_solver=\"randomized\", random_state=random_state\n",
    "    ).fit_transform(norm_counts)\n",
    "    umap_dr = umap.UMAP(random_state=random_state, min_dist=0.5).fit_transform(\n",
    "        reduced_counts\n",
    "    )\n",
    "    # Ensure only looking at positively identified doublets\n",
    "    doublets = labels == 1\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(4, 4), dpi=150)\n",
    "    axes.scatter(\n",
    "        umap_dr[:, 0],\n",
    "        umap_dr[:, 1],\n",
    "        c=\"grey\",\n",
    "        cmap=plt.cm.tab20,\n",
    "        s=1,\n",
    "        label=\"predicted singlets\",\n",
    "    )\n",
    "    axes.scatter(\n",
    "        umap_dr[:, 0][doublets],\n",
    "        umap_dr[:, 1][doublets],\n",
    "        s=3,\n",
    "        c=\"black\",\n",
    "        label=\"predicted doublets\",\n",
    "    )\n",
    "    axes.axis(\"off\")\n",
    "    axes.legend(frameon=False)\n",
    "    axes.set_title(\n",
    "        \"{} doublets out of {} cells\\n {}% cross-type doublet rate\".format(\n",
    "            np.sum(doublets),\n",
    "            raw_counts.shape[0],\n",
    "            np.round(100 * np.sum(doublets) / raw_counts.shape[0], 2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if show is True:\n",
    "        plt.show()\n",
    "    if isinstance(save, str):\n",
    "        fig.savefig(save, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    return fig, umap_dr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18a866",
   "metadata": {},
   "source": [
    "#### def run_scDblFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scDblFinder(adata, force_reload=False, layer=None, n_core=20, max_memory_gb=64):\n",
    "    '''\n",
    "    adata: adata object to normalize\n",
    "    layer: layer to use for normalization. Default = None -> use .X\n",
    "    force_reload: Force transfer of count data to R\n",
    "    '''\n",
    "    \n",
    "    import rpy2\n",
    "    import rpy2.robjects as ro\n",
    "    import gc\n",
    "    import doubletdetection\n",
    "\n",
    "       \n",
    "    print('Finding doublets with scDblFinder:')\n",
    "    # load packages\n",
    "    ro.globalenv['n_core'] = n_core\n",
    "    ro.globalenv['max_memory'] = max_memory_gb #/n_core\n",
    "    ro.r('''\n",
    "    print(paste0(\"Cores: \", n_core))\n",
    "    print(paste0(\"Memory: \", max_memory))\n",
    "    ''')\n",
    "    ro.r('''\n",
    "\n",
    "    # Analysis\n",
    "    library(Seurat)\n",
    "    library(sctransform)\n",
    "    library(scDblFinder)\n",
    "    library(SingleCellExperiment)\n",
    "    library(scater)\n",
    "    library(pastecs)\n",
    "\n",
    "    # Parallelization\n",
    "    library(BiocParallel)\n",
    "    register(MulticoreParam(n_core, progressbar = TRUE))\n",
    "\n",
    "    library(future)\n",
    "    plan(multicore, workers = n_core)\n",
    "    options(future.globals.maxSize = max_memory * 1024^3) # for 50 Gb RAM\n",
    "    plan()\n",
    "\n",
    "    library(doParallel)\n",
    "    registerDoParallel(n_core)\n",
    "    ''')\n",
    "    # transfer data\n",
    "    print('\\tTransfer data...')\n",
    "    \n",
    "    # check if data is in R workspace\n",
    "    if ro.r('''exists('data_mat')''')[0] == 1:\n",
    "        # check if data has same shape\n",
    "        if ro.globalenv['data_mat'].shape == adata.X.T.shape:\n",
    "            load_data = False\n",
    "            print('\\t\\tFound data matrix of same shape. Skipping data transfer...')\n",
    "        else:\n",
    "            load_data = True\n",
    "    else:\n",
    "        load_data = True\n",
    "        \n",
    "    if force_reload:\n",
    "        load_data = True\n",
    "    \n",
    "    if load_data:\n",
    "        if layer is None:\n",
    "            ro.globalenv['data_mat'] = adata.X.T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        else:\n",
    "            print('\\tUsing layer \\'', layer,'\\'...')\n",
    "            ro.globalenv['data_mat'] = adata.layers[layer].T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        \n",
    "        ro.r('''\n",
    "        rownames(data_mat) <- var_names\n",
    "        colnames(data_mat) <- obs_names\n",
    "        ''') \n",
    "    # standart preprocessing\n",
    "    ## create Seurat object    \n",
    "    ro.r('''\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    ''')   \n",
    "    ## preprocessing\n",
    "    print('\\tDoublet Detection with standard normalization...')\n",
    "    print('\\t\\tPreprocessing...')\n",
    "    ro.r('''\n",
    "    seurat <- NormalizeData(seurat, verbose = FALSE)\n",
    "    seurat <- FindVariableFeatures(seurat, selection.method = \"vst\", nfeatures = 5000, verbose = FALSE)\n",
    "    seurat <- ScaleData(seurat, verbose = FALSE)\n",
    "    seurat <- RunPCA(seurat, npcs = 50, verbose = FALSE)\n",
    "    seurat <- RunUMAP(seurat, reduction = \"pca\", dims = 1:50)\n",
    "    seurat <- FindNeighbors(seurat, dims = 1:50, verbose = FALSE)\n",
    "    seurat <- FindClusters(seurat, verbose = FALSE, resolution = 0.5)\n",
    "    #print(DimPlot(seurat, label = TRUE))\n",
    "    \n",
    "    #Conversion to SingleCellExperiment\n",
    "    sce <- as.SingleCellExperiment(seurat)\n",
    "    ''')\n",
    "    ## run scDblFinder\n",
    "    print('\\t\\tRunning scDblFinder...')\n",
    "    ro.r('''\n",
    "    #scDblFinder\n",
    "    colData(sce)$scoresDoubletDensity <- computeDoubletDensity(sce)\n",
    "    sce <- scDblFinder(sce, clusters = FALSE) #, dbr=0.1)\n",
    "    ''')\n",
    "    \n",
    "    \n",
    "    ## get results   \n",
    "    print('\\t\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results <- colData(sce)[,c(\"scDblFinder.class\", \"scDblFinder.score\")]    \n",
    "    ''')\n",
    "    \n",
    "    # sct preprocessing\n",
    "    ## create Seurat object    \n",
    "    ro.r('''\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    ''')   \n",
    "    ## preprocessing\n",
    "    print('\\tDoublet Detection with SCT normalization...')\n",
    "    print('\\t\\tPreprocessing...')\n",
    "    ro.r('''\n",
    "    seurat <- SCTransform(seurat, verbose = FALSE)\n",
    "    seurat <- RunPCA(seurat, npcs = 50, verbose = FALSE)\n",
    "    seurat <- RunUMAP(seurat, reduction = \"pca\", dims = 1:50)\n",
    "    seurat <- FindNeighbors(seurat, dims = 1:50, verbose = FALSE)\n",
    "    seurat <- FindClusters(seurat, verbose = FALSE, resolution = 0.5)\n",
    "    #print(DimPlot(seurat, label = TRUE))\n",
    "    \n",
    "    #Conversion to SingleCellExperiment\n",
    "    sce <- as.SingleCellExperiment(seurat)\n",
    "    ''')\n",
    "    ## run scDblFinder\n",
    "    print('\\t\\tRunning scDblFinder...')\n",
    "    ro.r('''\n",
    "    #scDblFinder\n",
    "    colData(sce)$scoresDoubletDensity <- computeDoubletDensity(sce)\n",
    "    sce <- scDblFinder(sce, clusters = FALSE) #, dbr=0.1)\n",
    "    ''')\n",
    "    \n",
    "    \n",
    "    ## get results   \n",
    "    print('\\t\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results <- cbind(results, colData(sce)[,c(\"scDblFinder.class\", \"scDblFinder.score\")])\n",
    "    colnames(results) <- c(\"scDblFinder.class\", \"scDblFinder.score\", \"scDblFinder.class.sct\", \"scDblFinder.score.sct\")\n",
    "    ''')\n",
    "    print('\\t\\tAdd results to anndata...')\n",
    "    results = ro.globalenv['results']\n",
    "    \n",
    "    # check if results are already present in adata.obs and delete\n",
    "    if 'scDblFinder.class' in adata.obs.columns:\n",
    "        del adata.obs[[\"scDblFinder.class.sct\", \"scDblFinder.score.sct\", \"scDblFinder.class.sct\", \"scDblFinder.score.sct\"]]\n",
    "    \n",
    "    adata.obs = pd.merge(adata.obs,results, left_index=True, right_index=True) #adata.obs[[\"scDblFinder.class.sct\", \"scDblFinder.score.sct\"]] = results.copy()\n",
    "    \n",
    "    adata.obs.loc[:,'sdf_doublets'] = False\n",
    "    adata.obs.loc[adata.obs.loc[:,'scDblFinder.class']=='doublet','sdf_doublets'] = True\n",
    "    adata.obs.loc[adata.obs.loc[:,'scDblFinder.class.sct']=='doublet','sdf_doublets'] = True\n",
    "    \n",
    "    print('\\n\\n------------------------------------------------------------------------------------\\n------------------------------------------------------------------------------------' )\n",
    "    print('\\nscDblFinder doublet rate:', adata.obs['sdf_doublets'].value_counts()[1]/adata.obs['sample'].value_counts()[0]*100, '% (',adata.obs['sdf_doublets'].value_counts()[1],' cells)' )\n",
    "    \n",
    "    #return adata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c9efd",
   "metadata": {},
   "source": [
    "#### def run_DoubletFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0175e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DoubletFinder(adata, force_reload=False, layer=None, n_core=20, max_memory_gb=64):\n",
    "    '''\n",
    "    adata: adata object to normalize\n",
    "    layer: layer to use for normalization. Default = None -> use .X\n",
    "    force_reload: Force transfer of count data to R\n",
    "    '''\n",
    "    \n",
    "    import rpy2\n",
    "    import rpy2.robjects as ro\n",
    "    import gc\n",
    "\n",
    "       \n",
    "    print('Finding doublets with scDblFinder:')\n",
    "    # load packages\n",
    "    ro.globalenv['n_core'] = n_core\n",
    "    ro.globalenv['max_memory'] = max_memory_gb #/n_core\n",
    "    ro.r('''\n",
    "    print(paste0(\"Cores: \", n_core))\n",
    "    print(paste0(\"Memory: \", max_memory))\n",
    "    ''')\n",
    "    ro.r('''\n",
    "\n",
    "    # Analysis\n",
    "    library(Seurat)\n",
    "    library(sctransform)\n",
    "    library(DoubletFinder)\n",
    "    library(SingleCellExperiment)\n",
    "    library(scater)\n",
    "    library(pastecs)\n",
    "\n",
    "    # Parallelization\n",
    "    library(BiocParallel)\n",
    "    register(MulticoreParam(n_core, progressbar = TRUE))\n",
    "\n",
    "    library(future)\n",
    "    plan(multicore, workers = n_core)\n",
    "    options(future.globals.maxSize = max_memory * 1024^3) # for 50 Gb RAM\n",
    "    plan()\n",
    "\n",
    "    library(doParallel)\n",
    "    registerDoParallel(n_core)\n",
    "    \n",
    "    # Adaption of original funtion to omit plot (https://github.com/chris-mcginnis-ucsf/DoubletFinder/blob/5dfd96b06365d7843adf3a72ffb6a30f42c74a01/R/find.pK.R)\n",
    "    find.pK.noPlot <- function(sweep.stats) {\n",
    "\n",
    "      ## Implementation for data without ground-truth doublet classifications \n",
    "      '%ni%' <- Negate('%in%')\n",
    "      if (\"AUC\" %ni% colnames(sweep.stats) == TRUE) {\n",
    "        ## Initialize data structure for results storage\n",
    "        bc.mvn <- as.data.frame(matrix(0L, nrow=length(unique(sweep.stats$pK)), ncol=5))\n",
    "        colnames(bc.mvn) <- c(\"ParamID\",\"pK\",\"MeanBC\",\"VarBC\",\"BCmetric\")\n",
    "        bc.mvn$pK <- unique(sweep.stats$pK)\n",
    "        bc.mvn$ParamID <- 1:nrow(bc.mvn)\n",
    "\n",
    "        ## Compute bimodality coefficient mean, variance, and BCmvn across pN-pK sweep results\n",
    "        x <- 0\n",
    "        for (i in unique(bc.mvn$pK)) {\n",
    "          x <- x + 1\n",
    "          ind <- which(sweep.stats$pK == i)\n",
    "          bc.mvn$MeanBC[x] <- mean(sweep.stats[ind, \"BCreal\"])\n",
    "          bc.mvn$VarBC[x] <- sd(sweep.stats[ind, \"BCreal\"])^2\n",
    "          bc.mvn$BCmetric[x] <- mean(sweep.stats[ind, \"BCreal\"])/(sd(sweep.stats[ind, \"BCreal\"])^2)\n",
    "        }\n",
    "\n",
    "        return(bc.mvn)\n",
    "\n",
    "      }\n",
    "\n",
    "      ## Implementation for data with ground-truth doublet classifications (e.g., MULTI-seq, CellHashing, Demuxlet, etc.)\n",
    "      if (\"AUC\" %in% colnames(sweep.stats) == TRUE) {\n",
    "        ## Initialize data structure for results storage\n",
    "        bc.mvn <- as.data.frame(matrix(0L, nrow=length(unique(sweep.stats$pK)), ncol=6))\n",
    "        colnames(bc.mvn) <- c(\"ParamID\",\"pK\",\"MeanAUC\",\"MeanBC\",\"VarBC\",\"BCmetric\")\n",
    "        bc.mvn$pK <- unique(sweep.stats$pK)\n",
    "        bc.mvn$ParamID <- 1:nrow(bc.mvn)\n",
    "\n",
    "        ## Compute bimodality coefficient mean, variance, and BCmvn across pN-pK sweep results\n",
    "        x <- 0\n",
    "        for (i in unique(bc.mvn$pK)) {\n",
    "          x <- x + 1\n",
    "          ind <- which(sweep.stats$pK == i)\n",
    "          bc.mvn$MeanAUC[x] <- mean(sweep.stats[ind, \"AUC\"])\n",
    "          bc.mvn$MeanBC[x] <- mean(sweep.stats[ind, \"BCreal\"])\n",
    "          bc.mvn$VarBC[x] <- sd(sweep.stats[ind, \"BCreal\"])^2\n",
    "          bc.mvn$BCmetric[x] <- mean(sweep.stats[ind, \"BCreal\"])/(sd(sweep.stats[ind, \"BCreal\"])^2)\n",
    "        }\n",
    "\n",
    "        return(bc.mvn)\n",
    "\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    ''')\n",
    "    # transfer data\n",
    "    print('\\tTransfer data...')\n",
    "    \n",
    "    # check if data is in R workspace\n",
    "    if ro.r('''exists('data_mat')''')[0] == 1:\n",
    "        # check if data has same shape\n",
    "        if ro.globalenv['data_mat'].shape == adata.X.T.shape:\n",
    "            load_data = False\n",
    "            print('\\t\\tFound data matrix of same shape. Skipping data transfer...')\n",
    "        else:\n",
    "            load_data = True\n",
    "    else:\n",
    "        load_data = True\n",
    "        \n",
    "    if force_reload:\n",
    "        load_data = True\n",
    "    \n",
    "    if load_data:\n",
    "        if layer is None:\n",
    "            ro.globalenv['data_mat'] = adata.X.T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        else:\n",
    "            print('\\tUsing layer \\'', layer,'\\'...')\n",
    "            ro.globalenv['data_mat'] = adata.layers[layer].T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        \n",
    "        ro.r('''\n",
    "        rownames(data_mat) <- var_names\n",
    "        colnames(data_mat) <- obs_names\n",
    "        ''') \n",
    "    # standart preprocessing\n",
    "    ## create Seurat object    \n",
    "    ro.r('''\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    ''')   \n",
    "    ## preprocessing\n",
    "    print('\\tDoublet Detection with standard normalization...')\n",
    "    print('\\t\\tPreprocessing...')\n",
    "    ro.r('''\n",
    "    seurat <- NormalizeData(seurat, verbose = FALSE)\n",
    "    seurat <- FindVariableFeatures(seurat, selection.method = \"vst\", nfeatures = 5000, verbose = FALSE)\n",
    "    seurat <- ScaleData(seurat, verbose = FALSE)\n",
    "    seurat <- RunPCA(seurat, npcs = 50, verbose = FALSE)\n",
    "    seurat <- RunUMAP(seurat, reduction = \"pca\", dims = 1:50)\n",
    "    seurat <- FindNeighbors(seurat, dims = 1:50, verbose = FALSE)\n",
    "    seurat <- FindClusters(seurat, verbose = FALSE, resolution = 0.5)\n",
    "    #print(DimPlot(seurat, label = TRUE))\n",
    "    ''')\n",
    "    ## run DoubletFinder\n",
    "    print('\\t\\tRunning DoubletFinder...')\n",
    "    ro.r('''\n",
    "    ## pK Identification (no ground-truth) ---------------------------------------------------------------------------------------\n",
    "    sweep.res.list <- paramSweep(seurat, PCs = 1:50, num.cores = n_core, sct = FALSE)\n",
    "    sweep.stats <- summarizeSweep(sweep.res.list, GT = FALSE)\n",
    "    bcmvn <- find.pK.noPlot(sweep.stats)\n",
    "    \n",
    "    ## Homotypic Doublet Proportion Estimate -------------------------------------------------------------------------------------\n",
    "    homotypic.prop <- modelHomotypic(seurat@meta.data$seurat_clusters)           ## ex: annotations <- seurat.list[[1]]@meta.data$ClusteringResults\n",
    "    nExp_poi <- round(0.1*length(seurat@meta.data$seurat_clusters))  # I guess that doublet formation rate is higher than the ~7.5% estimated from 10x if doublets are present in input cell suspension -> set to 10%  ## Assuming 7.5% doublet formation rate - tailor for your dataset\n",
    "    nExp_poi.adj <- round(nExp_poi*(1-homotypic.prop))\n",
    "    \n",
    "    ## Run DoubletFinder with varying classification stringencies ----------------------------------------------------------------\n",
    "    seurat <- doubletFinder(seurat, \n",
    "                                              PCs = 1:50, \n",
    "                                              pN = 0.25, \n",
    "                                              pK = as.numeric(as.character(bcmvn$pK[which.max(bcmvn$BCmetric)])), \n",
    "                                              nExp = nExp_poi, \n",
    "                                              reuse.pANN = FALSE, \n",
    "                                              sct = FALSE)\n",
    "    \n",
    "    seurat <- doubletFinder(seurat, \n",
    "                                              PCs = 1:50, \n",
    "                                              pN = 0.25, \n",
    "                                              pK = as.numeric(as.character(bcmvn$pK[which.max(bcmvn$BCmetric)])), \n",
    "                                              nExp = nExp_poi.adj, \n",
    "                                              reuse.pANN = paste0(\"pANN_0.25_\",as.character(bcmvn$pK[which.max(bcmvn$BCmetric)]),\"_\",nExp_poi), \n",
    "                                              sct = FALSE)\n",
    "    ''')\n",
    "       \n",
    "    \n",
    "    ## get results   \n",
    "    print('\\t\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results <- seurat@meta.data[,6:8]\n",
    "    colnames(results) <- c(\"pANN\",\"DF_classifications_1\",\"DF_classifications_2\")\n",
    "    ''')\n",
    "    \n",
    "    # sct preprocessing\n",
    "    ## create Seurat object    \n",
    "    ro.r('''\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    ''')   \n",
    "    ## preprocessing\n",
    "    print('\\tDoublet Detection with SCT normalization...')\n",
    "    print('\\t\\tPreprocessing...')\n",
    "    ro.r('''\n",
    "    seurat <- SCTransform(seurat, verbose = FALSE)\n",
    "    seurat <- RunPCA(seurat, npcs = 50, verbose = FALSE)\n",
    "    seurat <- RunUMAP(seurat, reduction = \"pca\", dims = 1:50)\n",
    "    seurat <- FindNeighbors(seurat, dims = 1:50, verbose = FALSE)\n",
    "    seurat <- FindClusters(seurat, verbose = FALSE, resolution = 0.5)\n",
    "    #print(DimPlot(seurat, label = TRUE))\n",
    "    ''')\n",
    "    ## run DoubletFinder\n",
    "    print('\\t\\tRunning DoubletFinder...')\n",
    "    ro.r('''\n",
    "    ## pK Identification (no ground-truth) ---------------------------------------------------------------------------------------\n",
    "    sweep.res.list <- paramSweep(seurat, PCs = 1:50, num.cores = n_core, sct = TRUE)\n",
    "    sweep.stats <- summarizeSweep(sweep.res.list, GT = FALSE)\n",
    "    bcmvn <- find.pK.noPlot(sweep.stats)\n",
    "    \n",
    "    ## Homotypic Doublet Proportion Estimate -------------------------------------------------------------------------------------\n",
    "    homotypic.prop <- modelHomotypic(seurat@meta.data$seurat_clusters)           ## ex: annotations <- seurat.list[[1]]@meta.data$ClusteringResults\n",
    "    nExp_poi <- round(0.1*length(seurat@meta.data$seurat_clusters))  # I guess that doublet formation rate is higher than the ~7.5% estimated from 10x if doublets are present in input cell suspension -> set to 10%  ## Assuming 7.5% doublet formation rate - tailor for your dataset\n",
    "    nExp_poi.adj <- round(nExp_poi*(1-homotypic.prop))\n",
    "    \n",
    "    ## Run DoubletFinder with varying classification stringencies ----------------------------------------------------------------\n",
    "    seurat <- doubletFinder(seurat, \n",
    "                                              PCs = 1:50, \n",
    "                                              pN = 0.25, \n",
    "                                              pK = as.numeric(as.character(bcmvn$pK[which.max(bcmvn$BCmetric)])), \n",
    "                                              nExp = nExp_poi, \n",
    "                                              reuse.pANN = FALSE, \n",
    "                                              sct = TRUE)\n",
    "    \n",
    "    seurat <- doubletFinder(seurat, \n",
    "                                              PCs = 1:50, \n",
    "                                              pN = 0.25, \n",
    "                                              pK = as.numeric(as.character(bcmvn$pK[which.max(bcmvn$BCmetric)])), \n",
    "                                              nExp = nExp_poi.adj, \n",
    "                                              reuse.pANN = paste0(\"pANN_0.25_\",as.character(bcmvn$pK[which.max(bcmvn$BCmetric)]),\"_\",nExp_poi), \n",
    "                                              sct = TRUE)\n",
    "    ''')\n",
    "    \n",
    "    \n",
    "    ## get results   \n",
    "    print('\\t\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results <- cbind(results, seurat@meta.data[,8:10])\n",
    "    colnames(results) <- c(\"pANN\",\"DF_classifications_1\",\"DF_classifications_2\", \"pANN.sct\",\"DF_classifications_1.sct\",\"DF_classifications_2.sct\")\n",
    "    ''')\n",
    "    print('\\t\\tAdd results to anndata...')\n",
    "    results = ro.globalenv['results']\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        results = ro.conversion.get_conversion().rpy2py(results) \n",
    "    \n",
    "    # check if results are already present in adata.obs and delete\n",
    "    if 'pANN' in adata.obs.columns:\n",
    "        del adata.obs[[\"pANN\",\"DF_classifications_1\",\"DF_classifications_2\", \"pANN.sct\",\"DF_classifications_1.sct\",\"DF_classifications_2.sct\"]]\n",
    "    \n",
    "    adata.obs = pd.merge(adata.obs,results, left_index=True, right_index=True) \n",
    "    \n",
    "    adata.obs.loc[:,'df_doublets'] = False\n",
    "    adata.obs.loc[adata.obs.loc[:,'DF_classifications_1']=='Doublet','df_doublets'] = True\n",
    "    adata.obs.loc[adata.obs.loc[:,'DF_classifications_1.sct']=='Doublet','df_doublets'] = True\n",
    "    \n",
    "    print('\\n\\n------------------------------------------------------------------------------------\\n------------------------------------------------------------------------------------' )\n",
    "    print('\\nDoubletFinder doublet rate: ', adata.obs['df_doublets'].value_counts()[1]/adata.obs['sample'].value_counts()[0]*100, '% (',adata.obs['df_doublets'].value_counts()[1],' cells)' )\n",
    "    \n",
    "    #return adata   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a6d89",
   "metadata": {},
   "source": [
    "#### def run_scDblFinder_ATAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f90c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scDblFinder_ATAC(adata, repeats_file='/mnt/ssd/Genomes/mm10/Repeats/AMULET_Exclusion_List_Regions/AMULET_exclusion_regions_noChr.bed', nfeatures=25, dbr=0.1, force_reload=False, layer=None, n_core=20, max_memory_gb=64):\n",
    "    '''\n",
    "    adata: adata object to normalize\n",
    "    repeats_file: Path to BED file with repeats and other exclusion regions e.g. '/mnt/ssd/Genomes/mm10/Repeats/AMULET_Exclusion_List_Regions/AMULET_exclusion_regions_noChr.bed'\n",
    "    layer: layer to use for normalization. Default = None -> use .X\n",
    "    force_reload: Force transfer of count data to R\n",
    "    '''\n",
    "    \n",
    "    import rpy2\n",
    "    import rpy2.robjects as ro\n",
    "    import gc\n",
    "\n",
    "       \n",
    "    print('\\nFinding scATAC-seq doublets with scDblFinder:')\n",
    "    # load packages\n",
    "    ro.globalenv['dbr'] = dbr\n",
    "    ro.globalenv['nfeatures'] = nfeatures\n",
    "    ro.globalenv['repeats_file'] = repeats_file\n",
    "    ro.globalenv['n_core'] = n_core\n",
    "    ro.globalenv['max_memory'] = max_memory_gb #/n_core\n",
    "    ro.r('''\n",
    "    print(paste0(\"Cores: \", n_core))\n",
    "    print(paste0(\"Memory: \", max_memory))\n",
    "    ''')\n",
    "    ro.r('''\n",
    "\n",
    "    # Analysis\n",
    "    library(Seurat)\n",
    "    library(sctransform)\n",
    "    library(scDblFinder)\n",
    "    library(SingleCellExperiment)\n",
    "    library(scater)\n",
    "    library(pastecs)\n",
    "    library(GenomicRanges)\n",
    "\n",
    "    # Parallelization\n",
    "    library(BiocParallel)\n",
    "    register(MulticoreParam(n_core, progressbar = TRUE))\n",
    "\n",
    "    library(future)\n",
    "    plan(multicore, workers = n_core)\n",
    "    options(future.globals.maxSize = max_memory * 1024^3) # for 50 Gb RAM\n",
    "    plan()\n",
    "\n",
    "    library(doParallel)\n",
    "    registerDoParallel(n_core)\n",
    "    ''')\n",
    "    # transfer data\n",
    "    print('\\tTransfer data...')\n",
    "    \n",
    "    # check if data is in R workspace\n",
    "    if ro.r('''exists('data_mat')''')[0] == 1:\n",
    "        # check if data has same shape\n",
    "        if ro.globalenv['data_mat'].shape == adata.X.T.shape:\n",
    "            load_data = False\n",
    "            print('\\t\\tFound data matrix of same shape. Skipping data transfer...')\n",
    "        else:\n",
    "            load_data = True\n",
    "    else:\n",
    "        load_data = True\n",
    "        \n",
    "    if force_reload:\n",
    "        load_data = True\n",
    "    \n",
    "    if load_data:\n",
    "        if layer is None:\n",
    "            ro.globalenv['data_mat'] = adata.X.T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        else:\n",
    "            print('\\tUsing layer \\'', layer,'\\'...')\n",
    "            ro.globalenv['data_mat'] = adata.layers[layer].T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        \n",
    "        ro.r('''\n",
    "        rownames(data_mat) <- var_names\n",
    "        colnames(data_mat) <- obs_names\n",
    "        ''') \n",
    "        \n",
    "    # prepare exclusion list\n",
    "    ro.r('''\n",
    "    repeats <- read.delim(repeats_file, header=FALSE)\n",
    "    repeats <- makeGRangesFromDataFrame(repeats, seqnames.field = \"V1\", start.field = \"V2\", end.field = \"V3\")\n",
    "    #repeats <- GRanges(\"6\", IRanges(1000,2000))\n",
    "    \n",
    "    otherChroms <- GRanges(c(\"M\",\"chrM\",\"MT\",\"X\",\"Y\",\"chrX\",\"chrY\"),IRanges(1L,width=10^8))\n",
    "    \n",
    "    toExclude <- suppressWarnings(c(repeats, otherChroms))\n",
    "    ''')\n",
    "    \n",
    "    # get fragments file path\n",
    "    ro.globalenv['fragments'] = adata.uns['files']['fragments']\n",
    "#     ro.r('''\n",
    "#     fragments <- system.file(\"extdata\", \"example_fragments.tsv.gz\", package=\"scDblFinder\")\n",
    "#     ''')\n",
    "    \n",
    "    # create SingleCellExperiment object    \n",
    "    ro.r('''\n",
    "    sce <- SingleCellExperiment(assays=list(counts=data_mat))\n",
    "    ''')\n",
    "\n",
    "    ## run scDblFinder\n",
    "    print('\\tRunning scDblFinder...')\n",
    "    ro.r('''\n",
    "    #scDblFinder\n",
    "    colData(sce)$scoresDoubletDensity <- computeDoubletDensity(sce)\n",
    "\n",
    "    error <- 1\n",
    "    while(error == 1){\n",
    "        catch <- tryCatch(sce <- scDblFinder(sce, aggregateFeatures=TRUE, nfeatures=nfeatures, processing=\"normFeatures\", dbr=dbr),\n",
    "                error=function(error){\n",
    "                warning(error)\n",
    "                return(list(sce, catch = \"FAILED\"))\n",
    "                })\n",
    "        if (is.null(catch[[\"catch\"]])) {\n",
    "          error <- 0\n",
    "          print(paste0(\"\\t\\tscDblFinder finished with \",nfeatures,\" features...\"))\n",
    "          rm(catch)\n",
    "        } else if (catch[[\"catch\"]] == \"FAILED\"){\n",
    "            print(paste0(\"\\t\\tscDblFinder failed with \",nfeatures,\" features!\"))\n",
    "            nfeatures <- nfeatures + 5\n",
    "        }\n",
    "    }\n",
    "    gc(verbose = TRUE, reset = FALSE, full = TRUE)\n",
    "    ''')   \n",
    "    \n",
    "    ## run AMULET\n",
    "    print('\\tRunning AMULET...')\n",
    "    ro.r('''\n",
    "    #AMULET\n",
    "    results <- amulet(fragments, regionsToExclude=toExclude, fullInMemory=TRUE)#, BPPARAM=MulticoreParam(n_core))\n",
    "    colnames(results) <- paste0('amulet.', colnames(results))\n",
    "    gc(verbose = TRUE, reset = FALSE, full = TRUE)\n",
    "    ''')\n",
    "    \n",
    "    ## get results   \n",
    "    print('\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results$atac.scDblFinder.p <- 1-colData(sce)[row.names(results), \"scDblFinder.score\"]\n",
    "    results$atac.combined <- apply(results[,c(\"atac.scDblFinder.p\", \"amulet.p.value\")], 1, FUN=function(x){\n",
    "      x[x<0.001] <- 0.001 # prevent too much skew from very small or 0 p-values\n",
    "      suppressWarnings(aggregation::fisher(x))\n",
    "    })\n",
    "    results$atac.combined.score <- -results$atac.combined + 1\n",
    "    #results$atac.combined.score <- -results$amulet.p.value + 1\n",
    "    \n",
    "    results$atac.combined.class <- doubletThresholding(data.frame('score'=results$atac.combined.score), dbr=dbr)\n",
    "    gc(verbose = TRUE, reset = FALSE, full = TRUE)\n",
    "    ''')\n",
    "    \n",
    "    \n",
    "    print('\\tAdd results to anndata...')\n",
    "    results = ro.globalenv['results']\n",
    "    with (ro.default_converter + pandas2ri.converter).context():\n",
    "        results = ro.conversion.get_conversion().rpy2py(results) \n",
    "    \n",
    "    # check if results are already present in adata.obs and delete\n",
    "    if results.columns[0] in adata.obs.columns:\n",
    "        del adata.obs[results.columns]\n",
    "    \n",
    "    adata.obs = pd.merge(adata.obs,results, left_index=True, right_index=True) #adata.obs[[\"scDblFinder.class.sct\", \"scDblFinder.score.sct\"]] = results.copy()\n",
    "    \n",
    "#     adata.obs.loc[:,'atac_sdf_doublets'] = False\n",
    "#     adata.obs.loc[adata.obs.loc[:,'atac.combined.class']=='doublet','sdf_doublets'] = True\n",
    "    \n",
    "#     print('\\n\\n------------------------------------------------------------------------------------\\n------------------------------------------------------------------------------------' )\n",
    "#     print('\\nATAC scDblFinder doublet rate:', adata.obs['atac_sdf_doublets'].value_counts()[1]/adata.obs['sample'].value_counts()[0]*100, '% (',adata.obs['atac_sdf_doublets'].value_counts()[1],' cells)' )\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147adbc9",
   "metadata": {},
   "source": [
    "#### def run_SCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83061dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SCDS(adata, force_reload=False, layer=None, n_core=20, max_memory_gb=64):\n",
    "    '''\n",
    "    adata: adata object to normalize\n",
    "    layer: layer to use for normalization. Default = None -> use .X\n",
    "    force_reload: Force transfer of count data to R\n",
    "    '''\n",
    "    \n",
    "    import rpy2\n",
    "    import rpy2.robjects as ro\n",
    "    import gc\n",
    "\n",
    "       \n",
    "    print('Finding doublets with scDblFinder:')\n",
    "    # load packages\n",
    "    ro.globalenv['n_core'] = n_core\n",
    "    ro.globalenv['max_memory'] = max_memory_gb #/n_core\n",
    "    ro.r('''\n",
    "    print(paste0(\"Cores: \", n_core))\n",
    "    print(paste0(\"Memory: \", max_memory))\n",
    "    ''')\n",
    "    ro.r('''\n",
    "\n",
    "    # Analysis\n",
    "    library(Seurat)\n",
    "    library(sctransform)\n",
    "    library(scds)\n",
    "    library(SingleCellExperiment)\n",
    "    library(scater)\n",
    "    library(pastecs)\n",
    "\n",
    "    # Parallelization\n",
    "    library(BiocParallel)\n",
    "    register(MulticoreParam(n_core, progressbar = TRUE))\n",
    "\n",
    "    library(future)\n",
    "    plan(multicore, workers = n_core)\n",
    "    options(future.globals.maxSize = max_memory * 1024^3) # for 50 Gb RAM\n",
    "    plan()\n",
    "\n",
    "    library(doParallel)\n",
    "    registerDoParallel(n_core)\n",
    "    ''')\n",
    "    # transfer data\n",
    "    print('\\tTransfer data...')\n",
    "    \n",
    "    # check if data is in R workspace\n",
    "    if ro.r('''exists('data_mat')''')[0] == 1:\n",
    "        # check if data has same shape\n",
    "        if ro.globalenv['data_mat'].shape == adata.X.T.shape:\n",
    "            load_data = False\n",
    "            print('\\t\\tFound data matrix of same shape. Skipping data transfer...')\n",
    "        else:\n",
    "            load_data = True\n",
    "    else:\n",
    "        load_data = True\n",
    "        \n",
    "    if force_reload:\n",
    "        load_data = True\n",
    "    \n",
    "    if load_data:\n",
    "        if layer is None:\n",
    "            ro.globalenv['data_mat'] = adata.X.T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        else:\n",
    "            print('\\tUsing layer \\'', layer,'\\'...')\n",
    "            ro.globalenv['data_mat'] = adata.layers[layer].T#.toarray()\n",
    "            ro.globalenv['obs_names'] = adata.obs_names\n",
    "            ro.globalenv['var_names'] = adata.var_names\n",
    "        \n",
    "        ro.r('''\n",
    "        rownames(data_mat) <- var_names\n",
    "        colnames(data_mat) <- obs_names\n",
    "        ''') \n",
    "    # standart preprocessing\n",
    "    ## create Seurat object    \n",
    "    ro.r('''\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    ''')   \n",
    "    ## preprocessing\n",
    "    print('\\tDoublet Detection with standard normalization...')\n",
    "    print('\\t\\tPreprocessing...')\n",
    "    ro.r('''\n",
    "    seurat <- NormalizeData(seurat, verbose = FALSE)\n",
    "    seurat <- FindVariableFeatures(seurat, selection.method = \"vst\", nfeatures = 5000, verbose = FALSE)\n",
    "    seurat <- ScaleData(seurat, verbose = FALSE)\n",
    "    seurat <- RunPCA(seurat, npcs = 50, verbose = FALSE)\n",
    "    seurat <- RunUMAP(seurat, reduction = \"pca\", dims = 1:50)\n",
    "    seurat <- FindNeighbors(seurat, dims = 1:50, verbose = FALSE)\n",
    "    seurat <- FindClusters(seurat, verbose = FALSE, resolution = 0.5)\n",
    "    #print(DimPlot(seurat, label = TRUE))\n",
    "    \n",
    "    #Conversion to SingleCellExperiment\n",
    "    sce <- as.SingleCellExperiment(seurat)\n",
    "    ''')\n",
    "    ## run SCDS\n",
    "    print('\\t\\tRunning SCDS...')\n",
    "    ro.r('''\n",
    "    # SCDS\n",
    "    sce <- cxds(sce, retRes = TRUE)\n",
    "    sce <- bcds(sce, retRes = TRUE, verb=TRUE)\n",
    "    sce <- cxds_bcds_hybrid(sce)\n",
    "    \n",
    "    dens <- density(sce$hybrid_score)\n",
    "    min_idx <- match(-1, extract(turnpoints(dens$y, calc.proba = TRUE)))\n",
    "    cut_off <- dens$x[min_idx[length(min_idx)]]\n",
    "    \n",
    "    #print(ggplot(as.data.frame(colData(sce)), aes(x=hybrid_score)) + geom_density() + geom_vline(xintercept = cut_off, linetype=2))\n",
    "    \n",
    "    sce$hybrid_class <- \"doublet\"\n",
    "    sce[,sce$hybrid_score < cut_off]$hybrid_class <- \"singlet\"\n",
    "    ''')\n",
    "    \n",
    "    \n",
    "    ## get results   \n",
    "    print('\\t\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results <- colData(sce)[,c(\"hybrid_class\", \"hybrid_score\")]    \n",
    "    ''')\n",
    "    \n",
    "    # sct preprocessing\n",
    "    ## create Seurat object    \n",
    "    ro.r('''\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    ''')   \n",
    "    ## preprocessing\n",
    "    print('\\tDoublet Detection with SCT normalization...')\n",
    "    print('\\t\\tPreprocessing...')\n",
    "    ro.r('''\n",
    "    seurat <- SCTransform(seurat, verbose = TRUE)\n",
    "    seurat <- RunPCA(seurat, npcs = 50, verbose = FALSE)\n",
    "    seurat <- RunUMAP(seurat, reduction = \"pca\", dims = 1:50)\n",
    "    seurat <- FindNeighbors(seurat, dims = 1:50, verbose = FALSE)\n",
    "    seurat <- FindClusters(seurat, verbose = FALSE, resolution = 0.5)\n",
    "    #print(DimPlot(seurat, label = TRUE))\n",
    "    \n",
    "    #Conversion to SingleCellExperiment\n",
    "    sce <- as.SingleCellExperiment(seurat)\n",
    "    ''')\n",
    "    ## run SCDS\n",
    "    print('\\t\\tRunning SCDS...')\n",
    "    ro.r('''\n",
    "    # SCDS\n",
    "    sce <- cxds(sce, retRes = TRUE)\n",
    "    sce <- bcds(sce, retRes = TRUE, verb=TRUE)\n",
    "    sce <- cxds_bcds_hybrid(sce)\n",
    "    \n",
    "    dens <- density(sce$hybrid_score)\n",
    "    min_idx <- match(-1, extract(turnpoints(dens$y, calc.proba = TRUE)))\n",
    "    cut_off <- dens$x[min_idx[length(min_idx)]]\n",
    "    \n",
    "    #print(ggplot(as.data.frame(colData(sce)), aes(x=hybrid_score)) + geom_density() + geom_vline(xintercept = cut_off, linetype=2))\n",
    "    \n",
    "    sce$hybrid_class <- \"doublet\"\n",
    "    sce[,sce$hybrid_score < cut_off]$hybrid_class <- \"singlet\"\n",
    "    ''')\n",
    "    \n",
    "    \n",
    "    ## get results   \n",
    "    print('\\t\\tCollect results...')\n",
    "    ro.r('''\n",
    "    results <- cbind(results, colData(sce)[,c(\"hybrid_class\", \"hybrid_score\")])\n",
    "    colnames(results) <- c(\"hybrid_class\", \"hybrid_score\", \"hybrid_class_sct\", \"hybrid_score_sct\")\n",
    "    ''')\n",
    "    print('\\t\\tAdd results to anndata...')\n",
    "    results = ro.globalenv['results']\n",
    "    \n",
    "    # check if results are already present in adata.obs and delete\n",
    "    if 'hybrid_class' in adata.obs.columns:\n",
    "        del adata.obs[[\"hybrid_class\", \"hybrid_score\", \"hybrid_class_sct\", \"hybrid_score_sct\"]]\n",
    "    \n",
    "    adata.obs = pd.merge(adata.obs,results, left_index=True, right_index=True) \n",
    "    \n",
    "    adata.obs.loc[:,'scds_doublets'] = False\n",
    "    adata.obs.loc[adata.obs.loc[:,'hybrid_class']=='doublet','scds_doublets'] = True\n",
    "    adata.obs.loc[adata.obs.loc[:,'hybrid_class_sct']=='doublet','scds_doublets'] = True\n",
    "    \n",
    "    print('\\n\\n------------------------------------------------------------------------------------\\n------------------------------------------------------------------------------------' )\n",
    "    print('\\nScds doublet rate:', adata.obs['scds_doublets'].value_counts()[1]/adata.obs['sample'].value_counts()[0]*100, '% (',adata.obs['scds_doublets'].value_counts()[1],' cells)' )\n",
    "    \n",
    "#     return adata\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a1b3a",
   "metadata": {},
   "source": [
    "## cell cycle genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cell_cycle_genes(adata, genome='mus musculus'):\n",
    "    # Load cell cycle genes\n",
    "\n",
    "    ## KEGG cell cycle genes\n",
    "    cc_kegg = pd.read_table('/mnt/hdd/data/KEGG_mmu_Cell_Cycle.txt').iloc[:,0].tolist()\n",
    "\n",
    "    ## Cell cycle genes Regev lab (Tirosh et al. 2016, DOI: 10.1126/science.aad0501)\n",
    "    cc_genes_regev = [x.strip() for x in open('/mnt/hdd/data/regev_cell_cycle_genes.txt')]\n",
    "        \n",
    "    if genome=='auto':\n",
    "        genome = '_'.join(adata.var.loc[:,'genome'][0].split('_')[0:2])\n",
    "    \n",
    "    print('Genome is', genome)\n",
    "        \n",
    "    if (genome == 'Homo_sapiens') | (genome == 'homo_sapiens'):\n",
    "\n",
    "        s_genes_regev = adata.var_names[np.isin(adata.var_names, cc_genes_regev[:43])]\n",
    "        g2m_genes_regev = adata.var_names[np.isin(adata.var_names, cc_genes_regev[43:])]\n",
    "\n",
    "        cc_genes_regev = list(adata.var_names[np.isin(adata.var_names, cc_genes_regev)])\n",
    "\n",
    "        ## Cell cycle genes Macosko et al. 2015, https://doi.org/10.1016/j.cell.2015.05.002\n",
    "        cc_genes_macosko = pd.read_table('/mnt/ssd/Resources/Macosko_cell_cycle_genes.txt', delimiter='\\t')\n",
    "\n",
    "        s_genes_macosko = list(adata.var_names[np.isin(adata.var_names, cc_genes_macosko['S'].dropna())])\n",
    "        g2m_genes_macosko = list(adata.var_names[np.isin(adata.var_names, cc_genes_macosko['G2.M'].dropna())])\n",
    "        m_genes_macosko = list(adata.var_names[np.isin(adata.var_names, cc_genes_macosko['M'].dropna())])\n",
    "        mg1_genes_macosko = list(adata.var_names[np.isin(adata.var_names, cc_genes_macosko['M.G1'].dropna())])\n",
    "        g1s_genes_macosko = list(adata.var_names[np.isin(adata.var_names, cc_genes_macosko['IG1.S'].dropna())])\n",
    "\n",
    "        cc_genes_macosko = s_genes_macosko + g2m_genes_macosko + m_genes_macosko + mg1_genes_macosko + g1s_genes_macosko\n",
    "\n",
    "        ## Combine all\n",
    "        all_cc_genes = list(set(cc_kegg + cc_genes_regev + cc_genes_macosko))\n",
    "        \n",
    "        return all_cc_genes, s_genes_regev, g2m_genes_regev, cc_genes_regev, cc_genes_macosko, s_genes_macosko, g2m_genes_macosko, m_genes_macosko, mg1_genes_macosko, g1s_genes_macosko\n",
    "\n",
    "    elif (genome == 'Mus_musculus') | (genome == 'mus_musculus'):\n",
    "        \n",
    "        s_genes_regev = [gene.lower().capitalize() for gene in cc_genes_regev[:43]]\n",
    "        g2m_genes_regev = [gene.lower().capitalize() for gene in cc_genes_regev[43:]]\n",
    "\n",
    "        cc_genes_regev = [gene.lower().capitalize() for gene in cc_genes_regev]\n",
    "\n",
    "        ## Cell cycle genes Macosko et al. 2015, https://doi.org/10.1016/j.cell.2015.05.002\n",
    "        cc_genes_macosko = pd.read_table('/mnt/hdd/data/Macosko_cell_cycle_genes.txt', delimiter='\\t')\n",
    "\n",
    "        s_genes_macosko = [gene.lower().capitalize() for gene in list(cc_genes_macosko['S'].dropna())]\n",
    "        g2m_genes_macosko = [gene.lower().capitalize() for gene in list(cc_genes_macosko['G2.M'].dropna())]\n",
    "        m_genes_macosko = [gene.lower().capitalize() for gene in list(cc_genes_macosko['M'].dropna())]\n",
    "        mg1_genes_macosko = [gene.lower().capitalize() for gene in list(cc_genes_macosko['M.G1'].dropna())]\n",
    "        g1s_genes_macosko = [gene.lower().capitalize() for gene in list(cc_genes_macosko['IG1.S'].dropna())]\n",
    "\n",
    "        cc_genes_macosko = s_genes_macosko + g2m_genes_macosko + m_genes_macosko + mg1_genes_macosko + g1s_genes_macosko\n",
    "\n",
    "        ## Combine all\n",
    "        all_cc_genes = list(set(cc_kegg + cc_genes_regev + cc_genes_macosko))\n",
    "        \n",
    "        return all_cc_genes, s_genes_regev, g2m_genes_regev, cc_genes_regev, cc_genes_macosko, s_genes_macosko, g2m_genes_macosko, m_genes_macosko, mg1_genes_macosko, g1s_genes_macosko\n",
    "\n",
    "    elif (genome == 'Sus_scrofa') | (genome == 'sus_scrofa'):\n",
    "        \n",
    "        s_genes_regev = mdata.var_names[np.isin(mdata.var_names, cc_genes_regev[:43])]\n",
    "        g2m_genes_regev = mdata.var_names[np.isin(mdata.var_names, cc_genes_regev[43:])]\n",
    "\n",
    "        cc_genes_regev = list(mdata.var_names[np.isin(mdata.var_names, cc_genes_regev)])\n",
    "\n",
    "        ## Cell cycle genes Macosko et al. 2015, https://doi.org/10.1016/j.cell.2015.05.002\n",
    "        cc_genes_macosko = pd.read_table('/mnt/ssd/Resources/Macosko_cell_cycle_genes.txt', delimiter='\\t')\n",
    "\n",
    "        s_genes_macosko = list(mdata.var_names[np.isin(mdata.var_names, cc_genes_macosko['S'].dropna())])\n",
    "        g2m_genes_macosko = list(mdata.var_names[np.isin(mdata.var_names, cc_genes_macosko['G2.M'].dropna())])\n",
    "        m_genes_macosko = list(mdata.var_names[np.isin(mdata.var_names, cc_genes_macosko['M'].dropna())])\n",
    "        mg1_genes_macosko = list(mdata.var_names[np.isin(mdata.var_names, cc_genes_macosko['M.G1'].dropna())])\n",
    "        g1s_genes_macosko = list(mdata.var_names[np.isin(mdata.var_names, cc_genes_macosko['IG1.S'].dropna())])\n",
    "\n",
    "        cc_genes_macosko = s_genes_macosko + g2m_genes_macosko + m_genes_macosko + mg1_genes_macosko + g1s_genes_macosko\n",
    "\n",
    "        ## Combine all\n",
    "        all_cc_genes = list(set(cc_kegg + cc_genes_regev + cc_genes_macosko))\n",
    "        \n",
    "        return all_cc_genes, s_genes_regev, g2m_genes_regev, cc_genes_regev, cc_genes_macosko, s_genes_macosko, g2m_genes_macosko, m_genes_macosko, mg1_genes_macosko, g1s_genes_macosko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918378c",
   "metadata": {},
   "source": [
    "## Nomalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import gc\n",
    "\n",
    "def normalise_scran(adata, r = 0.5):\n",
    "    print('Normalization with Scran:')\n",
    "    print('\\tPreprocess data...\\n\\t-----------------------------------\\n ')\n",
    "    adata_pp = adata.copy()\n",
    "    sc.pp.normalize_total(adata_pp)#, exclude_highly_expressed=True) #sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6)\n",
    "    sc.pp.log1p(adata_pp)\n",
    "    sc.pp.pca(adata_pp)\n",
    "    sc.pp.neighbors(adata_pp)\n",
    "    sc.tl.leiden(adata_pp, key_added='groups', resolution=r)\n",
    "\n",
    "    ro.globalenv['data_mat'] = adata_pp.X.T\n",
    "    ro.globalenv['input_groups'] = adata_pp.obs['groups']\n",
    "\n",
    "    print('\\tCalculate size factors...')\n",
    "    ro.r('library(\"scran\")')\n",
    "    # calculate size factors\n",
    "    ro.r('''\n",
    "    size_factors = calculateSumFactors(data_mat, clusters=input_groups, min.mean=0.1)\n",
    "    ''')\n",
    "\n",
    "    print('\\tTransfer data...')\n",
    "    # add to andata.obs\n",
    "    adata.obs['size_factors'] = ro.r['size_factors']\n",
    "\n",
    "    print('\\tPlot results...')\n",
    "    # plot results\n",
    "    rcParams['figure.figsize']=(5,5)\n",
    "    sc.pl.scatter(adata, 'size_factors', 'n_counts')\n",
    "    sc.pl.scatter(adata, 'size_factors', 'n_genes')\n",
    "\n",
    "    sb.histplot(adata.obs['size_factors'], bins=100, kde=True)\n",
    "    plt.show()\n",
    "\n",
    "    print('\\tAdd results to anndata...')\n",
    "    #Keep the count data in a counts layer\n",
    "    if not 'raw_counts' in adata.layers.keys():\n",
    "        adata.layers['raw_counts'] = adata.X.copy()\n",
    "\n",
    "    #Logarithmize raw counts\n",
    "    if not 'log_raw_counts' in adata.layers.keys():\n",
    "        adata.layers['log_raw_counts'] = sc.pp.log1p(adata.layers['raw_counts'], copy=True)\n",
    "\n",
    "    #Normalize adata \n",
    "    adata.X /= adata.obs['size_factors'].values[:,None]\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    #Keep the normalized count data in a counts layer\n",
    "    adata.layers['scran_counts'] = adata.X.copy()\n",
    "\n",
    "    # delete\n",
    "    print('\\tClean up...')\n",
    "    del adata_pp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sct(adata, layer=None, results_to_X=None, min_cells=None, n_core=20, max_memory=64):\n",
    "    '''\n",
    "    adata: adata object to normalize\n",
    "    layer: layer to use for normalization. Default = None -> use .X\n",
    "    results_to_X: Set results layer to adata.X (e.g. 'sct_logcounts')\n",
    "    '''\n",
    "    \n",
    "    import rpy2\n",
    "    import rpy2.robjects as ro\n",
    "    import gc\n",
    "\n",
    "       \n",
    "    print('Normalization with SCT:')\n",
    "    # load packages\n",
    "    ro.globalenv['n_core'] = n_core\n",
    "    ro.globalenv['max_memory'] = max_memory\n",
    "    ro.r('''\n",
    "    # Packages\n",
    "    library(Seurat)\n",
    "    library(sctransform)\n",
    "    library(SingleCellExperiment)\n",
    "\n",
    "    # Parallelization\n",
    "    library(BiocParallel)\n",
    "    register(MulticoreParam(n_core, progressbar = TRUE))\n",
    "\n",
    "    library(future)\n",
    "    plan(multisession, workers = n_core) #change from multicore hoping it would work better\n",
    "    options(future.globals.maxSize = max_memory * 1024^3)\n",
    "    options(future.globals.onReference = \"error\")\n",
    "\n",
    "    library(doParallel)\n",
    "    registerDoParallel(n_core)\n",
    "    ''')\n",
    "    # transfer data\n",
    "    print('\\tTransfer data...')\n",
    "    if layer is None:\n",
    "        ro.globalenv['data_mat'] = adata.X.T#.toarray()\n",
    "        ro.globalenv['obs_names'] = adata.obs_names\n",
    "        ro.globalenv['var_names'] = adata.var_names\n",
    "    else:\n",
    "        print('\\tNormalizing layer \\'', layer,'\\'...')\n",
    "        ro.globalenv['data_mat'] = adata.layers[layer].T#.toarray()\n",
    "        ro.globalenv['obs_names'] = adata.obs_names\n",
    "        ro.globalenv['var_names'] = adata.var_names\n",
    "        \n",
    "    ro.r('''\n",
    "    rownames(data_mat) <- var_names\n",
    "    colnames(data_mat) <- obs_names\n",
    "    seurat <- CreateSeuratObject(counts = data_mat, project = \"0\", min.cells = 0, min.features = 0)\n",
    "    gc()\n",
    "    ''')\n",
    "    gc.collect() \n",
    "    # perform sct\n",
    "    print('\\tPerform SCT...')\n",
    "    ro.r('''\n",
    "    # SCTransform\n",
    "    seurat <- SCTransform(seurat, verbose = FALSE, return.only.var.genes = FALSE, variable.features.n = NULL, vst.flavor = \"v2\")\n",
    "    gc()\n",
    "    ''')\n",
    "    gc.collect() \n",
    "    # convert to singleCellExperiment\n",
    "    print('\\tConvert data...')\n",
    "    ro.r('''\n",
    "    # Add feature meta data (since Seurat v4 -> will be fixed?)\n",
    "    var <- c('detection_rate','gmean', 'variance', 'residual_variance')\n",
    "    seurat[[\"SCT\"]]@meta.features <- SCTResults(seurat[[\"SCT\"]], slot = \"feature.attributes\")[, var]\n",
    "    seurat[[\"SCT\"]]@meta.features$variable <- FALSE\n",
    "    seurat[[\"SCT\"]]@meta.features[VariableFeatures(seurat[[\"SCT\"]] ), \"variable\"] <- TRUE\n",
    "    colnames(seurat[[\"SCT\"]]@meta.features) <- paste0(\"sct.\", colnames(seurat[[\"SCT\"]]@meta.features) )\n",
    "\n",
    "    # Convert to SingleCellExperiment\n",
    "    sce <- as.SingleCellExperiment(seurat)\n",
    "\n",
    "    # Add feature meta data (since Seurat v4 -> will be fixed?)\n",
    "    rowData(sce) <- seurat[[\"SCT\"]]@meta.features\n",
    "\n",
    "    # Rename and add layers\n",
    "    SummarizedExperiment::assay(sce, i = 1) <- seurat[[\"SCT\"]]@counts\n",
    "    SummarizedExperiment::assay(sce, i = 2) <- seurat[[\"SCT\"]]@data\n",
    "    SummarizedExperiment::assay(sce, i = 3) <- seurat[[\"SCT\"]]@scale.data\n",
    "    #SummarizedExperiment::assay(sce, i = 4) <- seurat[[\"RNA\"]]@counts\n",
    "    SummarizedExperiment::assayNames(sce) <- c(\"sct_counts\", \"sct_logcounts\", \"sct_scale_data\")#, \"raw_counts\")\n",
    "    gc()\n",
    "    ''')\n",
    "    \n",
    "    # transfer data\n",
    "    print('\\tTransfer data...')\n",
    "    \n",
    "    # add to andata.obs\n",
    "    adata_sct = ro.globalenv['sce']\n",
    "    adata_sct.layers['sct_counts'] = adata_sct.X.copy()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # Harmonize var_names\n",
    "    ## Remove underscores\n",
    "    adata.var_names = ['-'.join(var_name.split('_')) for var_name in adata.var_names]\n",
    "    var_adata = set(adata.var_names)\n",
    "    var_sct = set(adata_sct.var_names)\n",
    "    var_intersect = list(var_adata.intersection(var_sct))\n",
    "    # Subset adata\n",
    "    adata = adata[:,var_intersect]\n",
    "    adata_sct = adata_sct[:,var_intersect]\n",
    "    \n",
    "    # Add SCT data\n",
    "    print('\\tAdd results to anndata...')\n",
    "    adata.layers['sct_counts'] = adata_sct.layers['sct_counts'].copy()\n",
    "    adata.layers['sct_logcounts'] = adata_sct.layers['sct_logcounts'].copy()\n",
    "    adata.layers['sct_scale_data'] = adata_sct.layers['sct_scale_data'].copy()\n",
    "    adata.var[['sct.detection_rate', 'sct.gmean', 'sct.variance', 'sct.residual_variance', 'sct.variable']] = adata_sct.var[['sct.detection_rate', 'sct.gmean', 'sct.variance', 'sct.residual_variance', 'sct.variable']].copy()\n",
    "\n",
    "    if results_to_X is not None:\n",
    "        print('\\tSet',results_to_X,' anndata.X...')\n",
    "        adata.X = adata.layers[results_to_X].copy()\n",
    "        \n",
    "    # Set HVGs\n",
    "    print('\\tSet HVGs...')\n",
    "    adata.var.loc[:,'highly_variable'] = [bool(i) for i in adata_sct.var['sct.variable']]\n",
    "    #hvgs = pd.Series(adata.var['sct.variable'][adata.var['sct.variable'] > 0].index) # use HVGs from sct\n",
    "    #adata.var['highly_variable']= False\n",
    "    #adata.var.loc[hvgs,'highly_variable'] = True\n",
    "    \n",
    "    if min_cells is not None:\n",
    "        # Filter genes: Min 20 cells - filters out 0 count genes\n",
    "        print('\\tFilter genes...')\n",
    "        sc.pp.filter_genes(adata, min_cells=min_cells)\n",
    "    \n",
    "    # delete\n",
    "    ro.r('''\n",
    "    rm(list = ls())\n",
    "    gc()\n",
    "    ''')\n",
    "      \n",
    "    del adata_sct\n",
    "    gc.collect()\n",
    "    \n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ebd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_sct(adata, sct_results_path=None, keep_raw_qc=False, set_adata_raw=True, min_cells=None):\n",
    "    \n",
    "    import rpy2\n",
    "    import rpy2.robjects as ro\n",
    "    import gc\n",
    "    \n",
    "    if sct_results_path is None:\n",
    "        Print('Specify \\'sct_results_path\\'...')\n",
    "        return\n",
    "    \n",
    "    print('Importing SCT results from',sct_results_path)\n",
    "    print('\\tReading file...')\n",
    "    ro.globalenv['sct_results_path'] = sct_results_path\n",
    "    ro.r('library(SingleCellExperiment)')\n",
    "    ro.r('sct <- readRDS(sct_results_path)')\n",
    "    \n",
    "    print('\\tTransfer data...')\n",
    "    adata_sct = ro.r['sct']\n",
    "    \n",
    "    adata_sct.layers['sct_counts'] = adata_sct.X.copy()\n",
    "    \n",
    "    #return adata_sct\n",
    "    \n",
    "    print('\\tAdd SCT results to AnnData...')\n",
    "    # Harmonize var_names\n",
    "    ## Remove underscores\n",
    "    adata.var_names = ['-'.join(var_name.split('_')) for var_name in adata.var_names]\n",
    "    var_adata = set(adata.var_names)\n",
    "    var_sct = set(adata_sct.var_names)\n",
    "    var_intersect = list(var_adata.intersection(var_sct))\n",
    "    # Subset adata\n",
    "    adata = adata[:,var_intersect]\n",
    "    adata_sct = adata_sct[:,var_intersect]\n",
    "    # Add SCT data\n",
    "    adata.layers['sct_counts'] = adata_sct.layers['sct_counts']\n",
    "    adata.layers['sct_logcounts'] = adata_sct.layers['sct_logcounts']\n",
    "    adata.layers['sct_scale_data'] = adata_sct.layers['sct_scale_data']\n",
    "    adata.var[['sct.detection_rate', 'sct.gmean', 'sct.variance', 'sct.residual_variance', 'sct.variable']] = adata_sct.var[['sct.detection_rate', 'sct.gmean', 'sct.variance', 'sct.residual_variance', 'sct.variable']]\n",
    "    \n",
    "    # Put X in a layer to keep it after merging\n",
    "    adata_sct.layers['sct_counts'] = adata_sct.X.copy()\n",
    "    \n",
    "    if keep_raw_qc:\n",
    "        print('\\tSave raw QC metrics...')\n",
    "        # Keep raw QC metrics & counts\n",
    "        adata.obs['mt_frac_raw'] = adata.obs['mt_frac']\n",
    "        adata.obs['rp_frac_raw'] = adata.obs['rp_frac']\n",
    "        adata.obs['n_genes_raw'] = adata.obs['n_genes']\n",
    "        adata.obs['log_genes_raw'] = adata.obs['log_genes']\n",
    "        adata.obs['n_counts_raw'] = adata.obs['n_counts']\n",
    "        adata.obs['log_counts_raw'] = adata.obs['log_counts']\n",
    "        \n",
    "    if 'raw_counts' not in list(adata.layers):\n",
    "        print('\\tSave AnnData.X to AnnData.layers[\\'raw_counts\\']...')\n",
    "        adata.layers['raw_counts'] = adata.X\n",
    "\n",
    "    print('\\tRecalculate QC metrics...')\n",
    "    # Set normalized counts as X for QC metrics\n",
    "    adata.X = adata.layers['sct_counts']\n",
    "    qc_metrics(adata, ambient=False, make_dense=True)\n",
    "    \n",
    "    print('\\tSet SCT log counts as AnnData.X...')\n",
    "    # Set log-normalized counts as X\n",
    "    adata.X = adata.layers['sct_logcounts'].copy()\n",
    "    \n",
    "    print('\\tSet highly variable genes from SCT...')\n",
    "    # Set HVGs from SCT\n",
    "    hvgs = pd.Series(adata.var['sct.variable'][adata.var['sct.variable'] > 0].index) # use HVGs from sct\n",
    "    adata.var['highly_variable']= False\n",
    "    adata.var.loc[hvgs,'highly_variable'] = True\n",
    "    print('\\n','\\tNumber of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable'])))\n",
    "    \n",
    "    if set_adata_raw:\n",
    "        print('\\tStore full AnnData in AnnData.raw...')\n",
    "        # Store the full data set in 'raw' as log-normalized data for statistical testing\n",
    "        adata.raw = adata\n",
    "    \n",
    "    if min_cells is not None:\n",
    "        print('\\tFilter genes detected in less than',min_cells,'cells...')\n",
    "        # Filter genes: Min 20 cells - filters out 0 count genes\n",
    "        sc.pp.filter_genes(adata, min_cells=min_cells)\n",
    "        print('Number of genes after filter: {:d}'.format(adata.n_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f19282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tfidf(atac, hvg=False, hvg_min_mean=0.05, hvg_max_mean=1.5, hvg_min_disp=0.5, remove_1st_lsi=True):\n",
    "    \n",
    "    from muon import atac as ac\n",
    "    print('Normalization with SCT:')\n",
    "    \n",
    "    print('\\tSave raw counts to .layers[\\'atac_raw_counts\\']...')\n",
    "    # Save original counts\n",
    "    if 'atac_raw_counts' not in list(atac.layers):\n",
    "        print('\\tSave AnnData.X to AnnData.layers[\\'atac_raw_counts\\']...')\n",
    "        atac.layers['atac_raw_counts'] = atac.X\n",
    "    \n",
    "    # TF-IDF normalization\n",
    "    print('\\tTF-IDF normalization...')\n",
    "    ac.pp.tfidf(atac, scale_factor=1e4, log_tf=False, log_idf=False, log_tfidf=True)\n",
    "    \n",
    "    if hvg:\n",
    "        # Feature selection\n",
    "        sc.pp.highly_variable_genes(atac, min_mean=hvg_min_mean, max_mean=hvg_max_mean, min_disp=hvg_min_disp)\n",
    "        sc.pl.highly_variable_genes(atac)\n",
    "        print('\\t\\tNumber of variable features: ', np.sum(atac.var.highly_variable))\n",
    "    \n",
    "    # Save to .raw\n",
    "    print('\\tSave to .raw...')\n",
    "    atac.raw = atac\n",
    "    \n",
    "    # LSI\n",
    "    print('\\tLSI...')\n",
    "    ac.tl.lsi(atac)\n",
    "    \n",
    "    if remove_1st_lsi:\n",
    "        # 1st dimension is often associated with number peaks/counts and should be removed\n",
    "        \n",
    "        # plot 1st lsi against counts/peaks\n",
    "        lims_x = []\n",
    "        lims_y = []\n",
    "        lims_line = []\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, constrained_layout=True, figsize=(8, 4))\n",
    "        # Plots\n",
    "        axs[0].scatter(atac.obsm['X_lsi'][:,0], y=atac.obs['log_counts_ATAC'], s=2, alpha=0.2, c=atac.obs['n_peaks_ATAC'], cmap='rocket')\n",
    "        axs[1].scatter(atac.obsm['X_lsi'][:,0], y=atac.obs['log_peaks_ATAC'], s=2, alpha=0.2, c=atac.obs['n_counts_ATAC'], cmap='rocket')\n",
    "\n",
    "        # Aesthetics\n",
    "        for i,ax in enumerate(axs):\n",
    "            lims_x.append(ax.get_xlim())\n",
    "            lims_y.append(ax.get_ylim())\n",
    "\n",
    "        axs[0].set_xlabel('LSI Dim. 1')\n",
    "        axs[0].set_ylabel('Counts')\n",
    "        axs[0].set_xlim(lims_x[0])\n",
    "        axs[0].set_ylim(lims_y[0])\n",
    "\n",
    "        axs[1].set_xlabel('LSI Dim. 1')\n",
    "        axs[1].set_ylabel('Peaks')\n",
    "        axs[1].set_xlim(lims_x[1])\n",
    "        axs[1].set_ylim(lims_y[1])\n",
    "        \n",
    "        # remove 1st component\n",
    "        atac.obsm['X_lsi'] = atac.obsm['X_lsi'][:,1:]\n",
    "        atac.varm[\"LSI\"] = atac.varm[\"LSI\"][:,1:]\n",
    "        atac.uns[\"lsi\"][\"stdev\"] = atac.uns[\"lsi\"][\"stdev\"][1:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
